{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zn5egebb-8Uy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.tseries import offsets\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z4lnrizm-8U8"
      },
      "outputs": [],
      "source": [
        "MODEL = 'micn'\n",
        "MODE = 'regre'\n",
        "DATA = 'Trading'\n",
        "FEATURES = 'S'\n",
        "FREQ = 'h'\n",
        "CONV_KERNEL = [12, 16]\n",
        "D_LAYERS = 1\n",
        "D_MODEL = 512\n",
        "SEQ_LEN = 96\n",
        "DATA_PATH = '5m_intraday_data.csv'\n",
        "TARGET = 'Close'\n",
        "ENC_IN = 1\n",
        "DEC_IN = 1\n",
        "C_OUT = 1\n",
        "LABEL_LEN = 96\n",
        "PRED_LEN = 96\n",
        "BATCH_SIZE= 32\n",
        "N_HEADS = 8\n",
        "DROPOUT = 0.05\n",
        "EMBED = 'timeF'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DECOMP_KERNEL = [13, 17]\n",
        "ISOMETRIC_KERNEL = [17, 13]\n",
        "INVERSE = False\n",
        "OUTPUT_ATTENTION = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNpZBeA2-8U9"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8hFt5JEL-8U_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
        "    if args.lradj=='type1':\n",
        "        lr_adjust = {epoch: args.learning_rate * (0.75 ** ((epoch-1) // 1))}\n",
        "    elif args.lradj=='type2':\n",
        "        lr_adjust = {\n",
        "            2: 5e-4, 4: 1e-4, 6: 5e-5, 8: 1e-5,\n",
        "            10: 5e-6, 15: 1e-6, 20: 5e-7\n",
        "        }\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), path+'/'+'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "class StandardScaler():\n",
        "    def __init__(self):\n",
        "        self.mean = 0.\n",
        "        self.std = 1.\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.mean = data.mean(0)\n",
        "        self.std = data.std(0)\n",
        "\n",
        "    def transform(self, data):\n",
        "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
        "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
        "        return (data - mean) / std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
        "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
        "        return (data * std) + mean\n",
        "\n",
        "\n",
        "\n",
        "class TimeFeature:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"()\"\n",
        "\n",
        "class SecondOfMinute(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.second / 59.0 - 0.5\n",
        "\n",
        "class MinuteOfHour(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.minute / 59.0 - 0.5\n",
        "\n",
        "class HourOfDay(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.hour / 23.0 - 0.5\n",
        "\n",
        "class DayOfWeek(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.dayofweek / 6.0 - 0.5\n",
        "\n",
        "class DayOfMonth(TimeFeature):\n",
        "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.day - 1) / 30.0 - 0.5\n",
        "\n",
        "class DayOfYear(TimeFeature):\n",
        "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
        "\n",
        "class MonthOfYear(TimeFeature):\n",
        "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.month - 1) / 11.0 - 0.5\n",
        "\n",
        "class WeekOfYear(TimeFeature):\n",
        "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
        "\n",
        "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
        "    \"\"\"\n",
        "    Returns a list of time features that will be appropriate for the given frequency string.\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_str\n",
        "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
        "    \"\"\"\n",
        "\n",
        "    features_by_offsets = {\n",
        "        offsets.YearEnd: [],\n",
        "        offsets.QuarterEnd: [MonthOfYear],\n",
        "        offsets.MonthEnd: [MonthOfYear],\n",
        "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
        "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Minute: [\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "        offsets.Second: [\n",
        "            SecondOfMinute,\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    offset = to_offset(freq_str)\n",
        "\n",
        "    for offset_type, feature_classes in features_by_offsets.items():\n",
        "        if isinstance(offset, offset_type):\n",
        "            return [cls() for cls in feature_classes]\n",
        "\n",
        "    supported_freq_msg = f\"\"\"\n",
        "    Unsupported frequency {freq_str}\n",
        "    The following frequencies are supported:\n",
        "        Y   - yearly\n",
        "            alias: A\n",
        "        M   - monthly\n",
        "        W   - weekly\n",
        "        D   - daily\n",
        "        B   - business days\n",
        "        H   - hourly\n",
        "        T   - minutely\n",
        "            alias: min\n",
        "        S   - secondly\n",
        "    \"\"\"\n",
        "    raise RuntimeError(supported_freq_msg)\n",
        "\n",
        "def time_features(dates, timeenc=1, freq='h'):\n",
        "    \"\"\"\n",
        "    > `time_features` takes in a `dates` dataframe with a 'dates' column and extracts the date down to `freq` where freq can be any of the following if `timeenc` is 0:\n",
        "    > * m - [month]\n",
        "    > * w - [month]\n",
        "    > * d - [month, day, weekday]\n",
        "    > * b - [month, day, weekday]\n",
        "    > * h - [month, day, weekday, hour]\n",
        "    > * t - [month, day, weekday, hour, *minute]\n",
        "    >\n",
        "    > If `timeenc` is 1, a similar, but different list of `freq` values are supported (all encoded between [-0.5 and 0.5]):\n",
        "    > * Q - [month]\n",
        "    > * M - [month]\n",
        "    > * W - [Day of month, week of year]\n",
        "    > * D - [Day of week, day of month, day of year]\n",
        "    > * B - [Day of week, day of month, day of year]\n",
        "    > * H - [Hour of day, day of week, day of month, day of year]\n",
        "    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n",
        "    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n",
        "\n",
        "    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n",
        "    \"\"\"\n",
        "    if timeenc==0:\n",
        "        dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
        "        dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
        "        dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
        "        dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
        "        dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
        "        dates['minute'] = dates.minute.map(lambda x:x//15)\n",
        "        print(\"dates\", dates)\n",
        "        freq_map = {\n",
        "            'y':[],'m':['month'],'w':['month'],'d':['month','day','weekday'],\n",
        "            'b':['month','day','weekday'],'h':['month','day','weekday','hour'],\n",
        "            't':['month','day','weekday','hour','minute'],\n",
        "        }\n",
        "        print(dates[freq_map[freq.lower()]].values)\n",
        "        return dates[freq_map[freq.lower()]].values\n",
        "    if timeenc==1:\n",
        "        dates = pd.to_datetime(dates.date.values)\n",
        "        return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)]).transpose(1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8BcNEVXK-8VB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trading_dataset(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='5m_intraday_data.csv',\n",
        "                 target='Close', scale=True, inverse=True, timeenc=0, freq='5T', cols=None):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        if size is None:\n",
        "            self.seq_len = SEQ_LEN  # Adjust based on your needs\n",
        "            self.label_len = LABEL_LEN\n",
        "            self.pred_len = PRED_LEN\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "\n",
        "        # Initialize dataset type\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols = cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        # Load the CSV file\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
        "\n",
        "        # Dropping Gmtoffset column\n",
        "        df_raw = df_raw.drop(columns=['Gmtoffset'])\n",
        "\n",
        "        # Strip any spaces in column names just in case\n",
        "        df_raw.columns = df_raw.columns.str.strip()\n",
        "\n",
        "        # Check if the target column exists\n",
        "        if self.target not in df_raw.columns:\n",
        "            raise ValueError(f\"Target column '{self.target}' not found in the dataset\")\n",
        "\n",
        "        # Check for NaN values and replace them (customize this based on how you want to handle NaNs)\n",
        "        if df_raw.isnull().values.any():\n",
        "            print(\"NaN values found in the dataset, replacing with forward fill (ffill)\")\n",
        "            df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
        "\n",
        "        # Select columns (features)\n",
        "        if self.cols:\n",
        "            cols = self.cols.copy()\n",
        "            if self.target in cols:\n",
        "                cols.remove(self.target)\n",
        "        else:\n",
        "            # Use all columns except Timestamp, Datetime, and target\n",
        "            cols = list(df_raw.columns)\n",
        "            cols.remove(self.target)  # Ensure the target column is in the list\n",
        "            cols.remove('Timestamp')  # Unix timestamp\n",
        "            cols.remove('Datetime')   # Human-readable date\n",
        "\n",
        "        # Reorder the dataframe to have Datetime, features, and target\n",
        "        df_raw = df_raw[['Datetime'] + cols + [self.target]]\n",
        "\n",
        "        # Train/Val/Test splits\n",
        "        num_train = int(len(df_raw) * 0.7)\n",
        "        num_test = int(len(df_raw) * 0.2)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        # Features and target data selection\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            # Multivariate: use all columns except target and Datetime\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            # Single feature: only use the target column\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        # Scaling the data if required\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]  # Only scale using the training data\n",
        "            train_data = np.nan_to_num(train_data)  # Replace NaNs and infinities with 0\n",
        "            self.scaler.fit(train_data)\n",
        "            data = self.scaler.transform(np.nan_to_num(df_data.values))  # Apply scaling to the entire data\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        # Processing the datetime column for timestamp encoding\n",
        "        df_stamp = df_raw[['Datetime']][border1:border2]\n",
        "        df_stamp = df_stamp.rename(columns={'Datetime': 'date'})  # Rename 'Datetime' to 'date' for compatibility\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp['date'])\n",
        "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq)\n",
        "\n",
        "        # Save the data for X (features) and y (target)\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]  # Original scale for y if inverse\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]  # Scaled version\n",
        "\n",
        "        # Store time-related features\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        # Ensure valid sequence boundaries to avoid out-of-bounds indexing\n",
        "        if s_end > len(self.data_x) or r_end > len(self.data_x):\n",
        "            raise IndexError(f\"Index out of bounds: s_end={s_end}, r_end={r_end}\")\n",
        "\n",
        "        # Getting sequences for X (input) and Y (target)\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = np.concatenate([self.data_x[r_begin:r_begin+self.label_len], self.data_y[r_begin+self.label_len:r_end]], 0)\n",
        "        else:\n",
        "            seq_y = self.data_x[r_begin:r_end]\n",
        "\n",
        "        # Getting sequences for time encoding\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        # Debugging print to check for NaN values in the sequences\n",
        "        if np.isnan(seq_x).any() or np.isnan(seq_y).any():\n",
        "            print(f\"NaN values detected at index {index}\")\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSg9ntzV-8VC",
        "outputId": "15dcac0c-36ef-474f-e5a0-41fdfab76330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN values found in the dataset, replacing with forward fill (ffill)\n",
            "dates                      date  month  day  weekday  hour  minute\n",
            "0     2023-02-09 14:30:00      2    9        3    14       2\n",
            "1     2023-02-09 14:35:00      2    9        3    14       2\n",
            "2     2023-02-09 14:40:00      2    9        3    14       2\n",
            "3     2023-02-09 14:45:00      2    9        3    14       3\n",
            "4     2023-02-09 14:50:00      2    9        3    14       3\n",
            "...                   ...    ...  ...      ...   ...     ...\n",
            "22703 2024-04-04 15:20:00      4    4        3    15       1\n",
            "22704 2024-04-04 15:25:00      4    4        3    15       1\n",
            "22705 2024-04-04 15:30:00      4    4        3    15       2\n",
            "22706 2024-04-04 15:35:00      4    4        3    15       2\n",
            "22707 2024-04-04 15:40:00      4    4        3    15       2\n",
            "\n",
            "[22708 rows x 6 columns]\n",
            "[[ 2  9  3 14]\n",
            " [ 2  9  3 14]\n",
            " [ 2  9  3 14]\n",
            " ...\n",
            " [ 4  4  3 15]\n",
            " [ 4  4  3 15]\n",
            " [ 4  4  3 15]]\n",
            "NaN values found in the dataset, replacing with forward fill (ffill)\n",
            "dates                      date  month  day  weekday  hour  minute\n",
            "22612 2024-04-03 14:20:00      4    3        2    14       1\n",
            "22613 2024-04-03 14:25:00      4    3        2    14       1\n",
            "22614 2024-04-03 14:30:00      4    3        2    14       2\n",
            "22615 2024-04-03 14:35:00      4    3        2    14       2\n",
            "22616 2024-04-03 14:40:00      4    3        2    14       2\n",
            "...                   ...    ...  ...      ...   ...     ...\n",
            "25948 2024-06-03 15:50:00      6    3        0    15       3\n",
            "25949 2024-06-03 15:55:00      6    3        0    15       3\n",
            "25950 2024-06-03 16:00:00      6    3        0    16       0\n",
            "25951 2024-06-03 16:05:00      6    3        0    16       0\n",
            "25952 2024-06-03 16:10:00      6    3        0    16       0\n",
            "\n",
            "[3341 rows x 6 columns]\n",
            "[[ 4  3  2 14]\n",
            " [ 4  3  2 14]\n",
            " [ 4  3  2 14]\n",
            " ...\n",
            " [ 6  3  0 16]\n",
            " [ 6  3  0 16]\n",
            " [ 6  3  0 16]]\n",
            "NaN values found in the dataset, replacing with forward fill (ffill)\n",
            "dates                      date  month  day  weekday  hour  minute\n",
            "25857 2024-05-31 14:50:00      5   31        4    14       3\n",
            "25858 2024-05-31 14:55:00      5   31        4    14       3\n",
            "25859 2024-05-31 15:00:00      5   31        4    15       0\n",
            "25860 2024-05-31 15:05:00      5   31        4    15       0\n",
            "25861 2024-05-31 15:10:00      5   31        4    15       0\n",
            "...                   ...    ...  ...      ...   ...     ...\n",
            "32436 2024-09-30 19:40:00      9   30        0    19       2\n",
            "32437 2024-09-30 19:45:00      9   30        0    19       3\n",
            "32438 2024-09-30 19:50:00      9   30        0    19       3\n",
            "32439 2024-09-30 19:55:00      9   30        0    19       3\n",
            "32440 2024-09-30 20:00:00      9   30        0    20       0\n",
            "\n",
            "[6584 rows x 6 columns]\n",
            "[[ 5 31  4 14]\n",
            " [ 5 31  4 14]\n",
            " [ 5 31  4 15]\n",
            " ...\n",
            " [ 9 30  0 19]\n",
            " [ 9 30  0 19]\n",
            " [ 9 30  0 20]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\1111285665.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\1111285665.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\1111285665.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
            "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_3004\\9620485.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(\n",
        "    Trading_dataset(flag='train', root_path=r\"data/ETT/\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True)\n",
        "\n",
        "vali_loader = DataLoader(\n",
        "    Trading_dataset(flag='val', root_path=r\"data/ETT/\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    Trading_dataset(flag='test', root_path=r\"data/ETT/\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TtcLZk_V-8VE"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4; hour_size = 24\n",
        "        weekday_size = 7; day_size = 32; month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
        "        if freq=='t':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "\n",
        "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:,:,3])\n",
        "        weekday_x = self.weekday_embed(x[:,:,2])\n",
        "        day_x = self.day_embed(x[:,:,1])\n",
        "        month_x = self.month_embed(x[:,:,0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
        "\n",
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "\n",
        "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7ipE6Et7-8VI"
      },
      "outputs": [],
      "source": [
        "class moving_avg(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block to highlight the trend of time series\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(moving_avg, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: batch,seq_len,channels\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "\n",
        "class series_decomp_multi(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp_multi, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.moving_avg = [moving_avg(kernel, stride=1) for kernel in kernel_size]\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = []\n",
        "        res = []\n",
        "        for func in self.moving_avg:\n",
        "            moving_avg = func(x)\n",
        "            moving_mean.append(moving_avg)\n",
        "            sea = x - moving_avg\n",
        "            res.append(sea)\n",
        "\n",
        "        sea = sum(res) / len(res)\n",
        "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
        "        return sea, moving_mean\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, filter_size, dropout_rate=0.1):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
        "\n",
        "        self.initialize_weight(self.layer1)\n",
        "        self.initialize_weight(self.layer2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "    def initialize_weight(self, x):\n",
        "        nn.init.xavier_uniform_(x.weight)\n",
        "        if x.bias is not None:\n",
        "            nn.init.constant_(x.bias, 0)\n",
        "\n",
        "\n",
        "class MIC(nn.Module):\n",
        "    \"\"\"\n",
        "    MIC layer to extract local and global features\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24], isometric_kernel=[18, 6], device='cuda'):\n",
        "        super(MIC, self).__init__()\n",
        "        self.src_mask = None\n",
        "        self.conv_kernel = conv_kernel\n",
        "        self.isometric_kernel = isometric_kernel\n",
        "        self.device = device\n",
        "\n",
        "        # isometric convolution\n",
        "        self.isometric_conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
        "                                                   kernel_size=i,padding=0,stride=1)\n",
        "                                        for i in isometric_kernel])\n",
        "\n",
        "        # downsampling convolution: padding=i//2, stride=i\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
        "                                             kernel_size=i,padding=i//2,stride=i)\n",
        "                                  for i in conv_kernel])\n",
        "\n",
        "        # upsampling convolution\n",
        "        self.conv_trans = nn.ModuleList([nn.ConvTranspose1d(in_channels=feature_size, out_channels=feature_size,\n",
        "                                                            kernel_size=i,padding=0,stride=i)\n",
        "                                        for i in conv_kernel])\n",
        "\n",
        "        self.decomp = nn.ModuleList([series_decomp(k) for k in decomp_kernel])\n",
        "        self.merge = torch.nn.Conv2d(in_channels=feature_size, out_channels=feature_size, kernel_size=(len(self.conv_kernel), 1))\n",
        "\n",
        "        self.fnn = FeedForwardNetwork(feature_size, feature_size*4, dropout)\n",
        "        self.fnn_norm = torch.nn.LayerNorm(feature_size)\n",
        "\n",
        "        self.norm = torch.nn.LayerNorm(feature_size)\n",
        "        self.act = torch.nn.Tanh()\n",
        "        self.drop = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def conv_trans_conv(self, input, conv1d, conv1d_trans, isometric):\n",
        "        batch, seq_len, channel = input.shape\n",
        "        x = input.permute(0, 2, 1)\n",
        "\n",
        "        # downsampling convolution\n",
        "        x1 = self.drop(self.act(conv1d(x)))\n",
        "        x = x1\n",
        "\n",
        "        # print(f\"debug: {x.shape} | {x1.shape}\")\n",
        "\n",
        "        # isometric convolution\n",
        "        zeros = torch.zeros((x.shape[0], x.shape[1], x.shape[2]-1), device=self.device)\n",
        "        x = torch.cat((zeros, x), dim=-1)\n",
        "        x = self.drop(self.act(isometric(x)))\n",
        "        x = self.norm((x+x1).permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        # upsampling convolution\n",
        "        x = self.drop(self.act(conv1d_trans(x)))\n",
        "        x = x[:, :, :seq_len]   # truncate\n",
        "\n",
        "        x = self.norm(x.permute(0, 2, 1) + input)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # multi-scale\n",
        "        multi = []\n",
        "        for i in range(len(self.conv_kernel)):\n",
        "            src_out, trend1 = self.decomp[i](src)\n",
        "            src_out = self.conv_trans_conv(src_out, self.conv[i], self.conv_trans[i], self.isometric_conv[i])\n",
        "            multi.append(src_out)\n",
        "\n",
        "        # merge\n",
        "        mg = torch.tensor([], device = self.device)\n",
        "        for i in range(len(self.conv_kernel)):\n",
        "            mg = torch.cat((mg, multi[i].unsqueeze(1)), dim=1)\n",
        "        mg = self.merge(mg.permute(0,3,1,2)).squeeze(-2).permute(0,2,1)\n",
        "\n",
        "        return self.fnn_norm(mg + self.fnn(mg))\n",
        "\n",
        "\n",
        "class Seasonal_Prediction(nn.Module):\n",
        "    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1,\n",
        "                conv_kernel=[2, 4], isometric_kernel=[18, 6], device='cuda'):\n",
        "        super(Seasonal_Prediction, self).__init__()\n",
        "\n",
        "        self.mic = nn.ModuleList([MIC(feature_size=embedding_size, n_heads=n_heads,\n",
        "                                                   decomp_kernel=decomp_kernel,conv_kernel=conv_kernel, isometric_kernel=isometric_kernel, device=device)\n",
        "                                      for i in range(d_layers)])\n",
        "\n",
        "        self.projection = nn.Linear(embedding_size, c_out)\n",
        "\n",
        "    def forward(self, dec):\n",
        "        for mic_layer in self.mic:\n",
        "            dec = mic_layer(dec)\n",
        "        return self.projection(dec)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "34fqlKvi-8VJ"
      },
      "outputs": [],
      "source": [
        "class MICN(nn.Module):\n",
        "    def __init__(self, dec_in, c_out, seq_len, label_len, out_len,\n",
        "                 d_model=512, n_heads=8,d_layers=2,\n",
        "                 dropout=0.0,embed='fixed', freq='h',\n",
        "                 device=torch.device('cuda:0'), mode='regre',\n",
        "                 decomp_kernel=[33], conv_kernel=[12, 24], isometric_kernel=[18, 6],):\n",
        "        super(MICN, self).__init__()\n",
        "\n",
        "        self.pred_len = out_len\n",
        "        self.seq_len = seq_len\n",
        "        self.c_out = c_out\n",
        "        self.decomp_kernel = decomp_kernel\n",
        "        self.mode = mode\n",
        "\n",
        "        self.decomp_multi = series_decomp_multi(decomp_kernel)\n",
        "\n",
        "        # embedding\n",
        "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
        "\n",
        "        self.conv_trans = Seasonal_Prediction(embedding_size=d_model, n_heads=n_heads, dropout=dropout,\n",
        "                                     d_layers=d_layers, decomp_kernel=decomp_kernel, c_out=c_out, conv_kernel=conv_kernel,\n",
        "                                     isometric_kernel=isometric_kernel, device=device)\n",
        "\n",
        "        self.regression = nn.Linear(seq_len, out_len)\n",
        "        self.regression.weight = nn.Parameter((1/out_len) * torch.ones([out_len, seq_len]), requires_grad=True)\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "\n",
        "        # trend-cyclical prediction block: regre or mean\n",
        "        if self.mode == 'regre':\n",
        "            seasonal_init_enc, trend = self.decomp_multi(x_enc)\n",
        "            trend = self.regression(trend.permute(0,2,1)).permute(0, 2, 1)\n",
        "        elif self.mode == 'mean':\n",
        "            mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
        "            seasonal_init_enc, trend = self.decomp_multi(x_enc)\n",
        "            trend = torch.cat([trend[:, -self.seq_len:, :], mean], dim=1)\n",
        "\n",
        "        # embedding\n",
        "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
        "        seasonal_init_dec = torch.cat([seasonal_init_enc[:, -self.seq_len:, :], zeros], dim=1)\n",
        "        dec_out = self.dec_embedding(seasonal_init_dec, x_mark_dec)\n",
        "\n",
        "        dec_out = self.conv_trans(dec_out)\n",
        "        dec_out = dec_out[:, -self.pred_len:, :] + trend[:, -self.pred_len:, :]\n",
        "        return dec_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Zg-6Cj-8VJ",
        "outputId": "f670116e-dc14-4248-8059-f87a75da91fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Started Model training =====\n",
            "\titers: 10, epoch: 1 | loss: 25884.5234375\n",
            "\tspeed: 0.3988s/iter; left time: 2519.3501s\n",
            "\titers: 20, epoch: 1 | loss: 25213.3378906\n",
            "\tspeed: 0.3933s/iter; left time: 2480.8384s\n",
            "\titers: 30, epoch: 1 | loss: 22994.3750000\n",
            "\tspeed: 0.4083s/iter; left time: 2571.2504s\n",
            "\titers: 40, epoch: 1 | loss: 20600.7402344\n",
            "\tspeed: 0.4163s/iter; left time: 2617.8771s\n",
            "\titers: 50, epoch: 1 | loss: 19152.1386719\n",
            "\tspeed: 0.4191s/iter; left time: 2630.9282s\n",
            "\titers: 60, epoch: 1 | loss: 17329.9785156\n",
            "\tspeed: 0.4408s/iter; left time: 2762.9150s\n",
            "\titers: 70, epoch: 1 | loss: 15109.6289062\n",
            "\tspeed: 0.4162s/iter; left time: 2604.7658s\n",
            "\titers: 80, epoch: 1 | loss: 12566.2451172\n",
            "\tspeed: 0.4106s/iter; left time: 2565.4349s\n",
            "\titers: 90, epoch: 1 | loss: 12563.6494141\n",
            "\tspeed: 0.4111s/iter; left time: 2564.4409s\n",
            "\titers: 100, epoch: 1 | loss: 10137.5126953\n",
            "\tspeed: 0.4123s/iter; left time: 2567.9374s\n",
            "\titers: 110, epoch: 1 | loss: 8859.2949219\n",
            "\tspeed: 0.4161s/iter; left time: 2587.4450s\n",
            "\titers: 120, epoch: 1 | loss: 7130.3906250\n",
            "\tspeed: 0.4331s/iter; left time: 2688.6547s\n",
            "\titers: 130, epoch: 1 | loss: 6367.6538086\n",
            "\tspeed: 0.4176s/iter; left time: 2588.2528s\n",
            "\titers: 140, epoch: 1 | loss: 5199.8540039\n",
            "\tspeed: 0.4178s/iter; left time: 2585.2915s\n",
            "\titers: 150, epoch: 1 | loss: 4450.1782227\n",
            "\tspeed: 0.4194s/iter; left time: 2590.9720s\n",
            "\titers: 160, epoch: 1 | loss: 3552.6984863\n",
            "\tspeed: 0.4186s/iter; left time: 2581.7078s\n",
            "\titers: 170, epoch: 1 | loss: 2595.8002930\n",
            "\tspeed: 0.4266s/iter; left time: 2627.1096s\n",
            "\titers: 180, epoch: 1 | loss: 2092.5246582\n",
            "\tspeed: 0.4177s/iter; left time: 2567.8818s\n",
            "\titers: 190, epoch: 1 | loss: 1397.5146484\n",
            "\tspeed: 0.4189s/iter; left time: 2571.2541s\n",
            "\titers: 200, epoch: 1 | loss: 1157.3389893\n",
            "\tspeed: 0.4174s/iter; left time: 2557.9172s\n",
            "\titers: 210, epoch: 1 | loss: 805.9960327\n",
            "\tspeed: 0.4236s/iter; left time: 2591.4805s\n",
            "\titers: 220, epoch: 1 | loss: 627.6654663\n",
            "\tspeed: 0.4202s/iter; left time: 2566.7531s\n",
            "\titers: 230, epoch: 1 | loss: 487.5202332\n",
            "\tspeed: 0.4184s/iter; left time: 2551.2725s\n",
            "\titers: 240, epoch: 1 | loss: 298.0350647\n",
            "\tspeed: 0.4214s/iter; left time: 2565.7340s\n",
            "\titers: 250, epoch: 1 | loss: 195.3985138\n",
            "\tspeed: 0.4218s/iter; left time: 2563.6957s\n",
            "\titers: 260, epoch: 1 | loss: 124.8431015\n",
            "\tspeed: 0.4205s/iter; left time: 2551.8914s\n",
            "\titers: 270, epoch: 1 | loss: 75.4455719\n",
            "\tspeed: 0.4190s/iter; left time: 2538.3745s\n",
            "\titers: 280, epoch: 1 | loss: 62.6272240\n",
            "\tspeed: 0.4202s/iter; left time: 2541.5320s\n",
            "\titers: 290, epoch: 1 | loss: 34.5664940\n",
            "\tspeed: 0.4238s/iter; left time: 2558.8672s\n",
            "\titers: 300, epoch: 1 | loss: 35.3261147\n",
            "\tspeed: 0.4238s/iter; left time: 2554.5313s\n",
            "\titers: 310, epoch: 1 | loss: 18.7940578\n",
            "\tspeed: 0.4272s/iter; left time: 2571.1492s\n",
            "\titers: 320, epoch: 1 | loss: 19.2891159\n",
            "\tspeed: 0.4200s/iter; left time: 2523.1977s\n",
            "\titers: 330, epoch: 1 | loss: 12.4714918\n",
            "\tspeed: 0.4214s/iter; left time: 2527.4718s\n",
            "\titers: 340, epoch: 1 | loss: 9.9428759\n",
            "\tspeed: 0.4204s/iter; left time: 2517.1563s\n",
            "\titers: 350, epoch: 1 | loss: 9.8825617\n",
            "\tspeed: 0.4212s/iter; left time: 2517.6469s\n",
            "\titers: 360, epoch: 1 | loss: 7.1562576\n",
            "\tspeed: 0.4212s/iter; left time: 2513.5942s\n",
            "\titers: 370, epoch: 1 | loss: 7.3497901\n",
            "\tspeed: 0.4211s/iter; left time: 2508.9436s\n",
            "\titers: 380, epoch: 1 | loss: 5.1923242\n",
            "\tspeed: 0.4208s/iter; left time: 2503.1163s\n",
            "\titers: 390, epoch: 1 | loss: 6.8742270\n",
            "\tspeed: 0.4236s/iter; left time: 2515.1637s\n",
            "\titers: 400, epoch: 1 | loss: 6.0940728\n",
            "\tspeed: 0.4217s/iter; left time: 2499.9562s\n",
            "\titers: 410, epoch: 1 | loss: 3.0816672\n",
            "\tspeed: 0.4270s/iter; left time: 2526.8065s\n",
            "\titers: 420, epoch: 1 | loss: 5.3733087\n",
            "\tspeed: 0.4207s/iter; left time: 2485.5603s\n",
            "\titers: 430, epoch: 1 | loss: 8.6011324\n",
            "\tspeed: 0.4233s/iter; left time: 2496.3391s\n",
            "\titers: 440, epoch: 1 | loss: 7.3940415\n",
            "\tspeed: 0.4209s/iter; left time: 2478.5355s\n",
            "\titers: 450, epoch: 1 | loss: 12.2776518\n",
            "\tspeed: 0.4271s/iter; left time: 2510.5883s\n",
            "\titers: 460, epoch: 1 | loss: 4.8562670\n",
            "\tspeed: 0.4229s/iter; left time: 2481.3871s\n",
            "\titers: 470, epoch: 1 | loss: 6.5115752\n",
            "\tspeed: 0.4222s/iter; left time: 2473.3520s\n",
            "\titers: 480, epoch: 1 | loss: 6.5202117\n",
            "\tspeed: 0.4253s/iter; left time: 2487.4064s\n",
            "\titers: 490, epoch: 1 | loss: 3.4392502\n",
            "\tspeed: 0.4243s/iter; left time: 2477.2750s\n",
            "\titers: 500, epoch: 1 | loss: 5.5129323\n",
            "\tspeed: 0.4296s/iter; left time: 2503.4641s\n",
            "\titers: 510, epoch: 1 | loss: 5.9505124\n",
            "\tspeed: 0.4211s/iter; left time: 2449.9214s\n",
            "\titers: 520, epoch: 1 | loss: 3.1812146\n",
            "\tspeed: 0.4266s/iter; left time: 2477.7550s\n",
            "\titers: 530, epoch: 1 | loss: 7.4148655\n",
            "\tspeed: 0.4251s/iter; left time: 2464.6230s\n",
            "\titers: 540, epoch: 1 | loss: 4.7512531\n",
            "\tspeed: 0.4208s/iter; left time: 2435.3813s\n",
            "\titers: 550, epoch: 1 | loss: 5.1461196\n",
            "\tspeed: 0.4301s/iter; left time: 2484.9305s\n",
            "\titers: 560, epoch: 1 | loss: 7.5571971\n",
            "\tspeed: 0.4209s/iter; left time: 2427.8327s\n",
            "\titers: 570, epoch: 1 | loss: 3.8092804\n",
            "\tspeed: 0.4236s/iter; left time: 2438.9275s\n",
            "\titers: 580, epoch: 1 | loss: 6.9358311\n",
            "\tspeed: 0.4238s/iter; left time: 2435.9501s\n",
            "\titers: 590, epoch: 1 | loss: 6.8958430\n",
            "\tspeed: 0.4230s/iter; left time: 2426.9072s\n",
            "\titers: 600, epoch: 1 | loss: 5.3441405\n",
            "\tspeed: 0.4320s/iter; left time: 2474.2256s\n",
            "\titers: 610, epoch: 1 | loss: 3.2834854\n",
            "\tspeed: 0.4216s/iter; left time: 2410.8829s\n",
            "\titers: 620, epoch: 1 | loss: 2.6572173\n",
            "\tspeed: 0.4229s/iter; left time: 2413.8693s\n",
            "\titers: 630, epoch: 1 | loss: 5.5244370\n",
            "\tspeed: 0.4239s/iter; left time: 2415.4321s\n",
            "\titers: 640, epoch: 1 | loss: 4.4003253\n",
            "\tspeed: 0.4284s/iter; left time: 2436.6883s\n",
            "\titers: 650, epoch: 1 | loss: 4.1086597\n",
            "\tspeed: 0.4237s/iter; left time: 2405.5593s\n",
            "\titers: 660, epoch: 1 | loss: 5.3842721\n",
            "\tspeed: 0.4256s/iter; left time: 2412.2879s\n",
            "\titers: 670, epoch: 1 | loss: 6.9801764\n",
            "\tspeed: 0.4239s/iter; left time: 2398.4749s\n",
            "\titers: 680, epoch: 1 | loss: 4.6248479\n",
            "\tspeed: 0.4233s/iter; left time: 2390.7377s\n",
            "\titers: 690, epoch: 1 | loss: 5.7078652\n",
            "\tspeed: 0.4275s/iter; left time: 2410.4929s\n",
            "\titers: 700, epoch: 1 | loss: 3.9812734\n",
            "\tspeed: 0.4231s/iter; left time: 2381.4641s\n",
            "Epoch: 1, Train Loss: 3394.1709, Validation Loss: 8.1988\n",
            "Validation loss decreased (inf --> 8.198812).  Saving model ...\n",
            "\titers: 10, epoch: 2 | loss: 3.5762942\n",
            "\tspeed: 0.4293s/iter; left time: 2410.2614s\n",
            "\titers: 20, epoch: 2 | loss: 4.6989408\n",
            "\tspeed: 0.4231s/iter; left time: 2371.6140s\n",
            "\titers: 30, epoch: 2 | loss: 4.2123098\n",
            "\tspeed: 0.4279s/iter; left time: 2393.8665s\n",
            "\titers: 40, epoch: 2 | loss: 3.6299574\n",
            "\tspeed: 0.4212s/iter; left time: 2352.5746s\n",
            "\titers: 50, epoch: 2 | loss: 3.6811140\n",
            "\tspeed: 0.4232s/iter; left time: 2359.5767s\n",
            "\titers: 60, epoch: 2 | loss: 6.9026999\n",
            "\tspeed: 0.4288s/iter; left time: 2386.4241s\n",
            "\titers: 70, epoch: 2 | loss: 6.0239162\n",
            "\tspeed: 0.4247s/iter; left time: 2359.3755s\n",
            "\titers: 80, epoch: 2 | loss: 3.1884708\n",
            "\tspeed: 0.4259s/iter; left time: 2361.6113s\n",
            "\titers: 90, epoch: 2 | loss: 3.7893841\n",
            "\tspeed: 0.4234s/iter; left time: 2343.5773s\n",
            "\titers: 100, epoch: 2 | loss: 4.3542943\n",
            "\tspeed: 0.4267s/iter; left time: 2357.3105s\n",
            "\titers: 110, epoch: 2 | loss: 4.3206382\n",
            "\tspeed: 0.4263s/iter; left time: 2351.0293s\n",
            "\titers: 120, epoch: 2 | loss: 3.1921885\n",
            "\tspeed: 0.4247s/iter; left time: 2337.8527s\n",
            "\titers: 130, epoch: 2 | loss: 5.5435257\n",
            "\tspeed: 0.4226s/iter; left time: 2322.4052s\n",
            "\titers: 140, epoch: 2 | loss: 4.8106933\n",
            "\tspeed: 0.4255s/iter; left time: 2333.9339s\n",
            "\titers: 150, epoch: 2 | loss: 2.9557209\n",
            "\tspeed: 0.4229s/iter; left time: 2315.5381s\n",
            "\titers: 160, epoch: 2 | loss: 6.5321126\n",
            "\tspeed: 0.4248s/iter; left time: 2321.4122s\n",
            "\titers: 170, epoch: 2 | loss: 4.3959050\n",
            "\tspeed: 0.4246s/iter; left time: 2316.3529s\n",
            "\titers: 180, epoch: 2 | loss: 7.9423709\n",
            "\tspeed: 0.4310s/iter; left time: 2346.9532s\n",
            "\titers: 190, epoch: 2 | loss: 3.9500778\n",
            "\tspeed: 0.4239s/iter; left time: 2304.0596s\n",
            "\titers: 200, epoch: 2 | loss: 5.0391436\n",
            "\tspeed: 0.4311s/iter; left time: 2338.8340s\n",
            "\titers: 210, epoch: 2 | loss: 3.4661529\n",
            "\tspeed: 0.4249s/iter; left time: 2300.7268s\n",
            "\titers: 220, epoch: 2 | loss: 7.5083046\n",
            "\tspeed: 0.4241s/iter; left time: 2292.4467s\n",
            "\titers: 230, epoch: 2 | loss: 4.2184100\n",
            "\tspeed: 0.4255s/iter; left time: 2295.7462s\n",
            "\titers: 240, epoch: 2 | loss: 5.8953424\n",
            "\tspeed: 0.4241s/iter; left time: 2283.7846s\n",
            "\titers: 250, epoch: 2 | loss: 3.9319098\n",
            "\tspeed: 0.4292s/iter; left time: 2307.1229s\n",
            "\titers: 260, epoch: 2 | loss: 3.3992198\n",
            "\tspeed: 0.4217s/iter; left time: 2262.5773s\n",
            "\titers: 270, epoch: 2 | loss: 7.0427060\n",
            "\tspeed: 0.4217s/iter; left time: 2258.0472s\n",
            "\titers: 280, epoch: 2 | loss: 8.2029152\n",
            "\tspeed: 0.4211s/iter; left time: 2250.6678s\n",
            "\titers: 290, epoch: 2 | loss: 2.9785702\n",
            "\tspeed: 0.4283s/iter; left time: 2284.9356s\n",
            "\titers: 300, epoch: 2 | loss: 4.4164033\n",
            "\tspeed: 0.4246s/iter; left time: 2260.8048s\n",
            "\titers: 310, epoch: 2 | loss: 4.6455054\n",
            "\tspeed: 0.4210s/iter; left time: 2237.7523s\n",
            "\titers: 320, epoch: 2 | loss: 5.7926641\n",
            "\tspeed: 0.4206s/iter; left time: 2231.1989s\n",
            "\titers: 330, epoch: 2 | loss: 2.8961360\n",
            "\tspeed: 0.4215s/iter; left time: 2231.8229s\n",
            "\titers: 340, epoch: 2 | loss: 5.4880681\n",
            "\tspeed: 0.4275s/iter; left time: 2259.4455s\n",
            "\titers: 350, epoch: 2 | loss: 5.0796475\n",
            "\tspeed: 0.4210s/iter; left time: 2221.0266s\n",
            "\titers: 360, epoch: 2 | loss: 3.9974506\n",
            "\tspeed: 0.4237s/iter; left time: 2230.9142s\n",
            "\titers: 370, epoch: 2 | loss: 4.8191295\n",
            "\tspeed: 0.4212s/iter; left time: 2213.2168s\n",
            "\titers: 380, epoch: 2 | loss: 4.9518285\n",
            "\tspeed: 0.4214s/iter; left time: 2210.3739s\n",
            "\titers: 390, epoch: 2 | loss: 7.5186539\n",
            "\tspeed: 0.4300s/iter; left time: 2251.2305s\n",
            "\titers: 400, epoch: 2 | loss: 5.0899210\n",
            "\tspeed: 0.4214s/iter; left time: 2201.7908s\n",
            "\titers: 410, epoch: 2 | loss: 3.4978931\n",
            "\tspeed: 0.4200s/iter; left time: 2190.2057s\n",
            "\titers: 420, epoch: 2 | loss: 6.0836482\n",
            "\tspeed: 0.4197s/iter; left time: 2184.4353s\n",
            "\titers: 430, epoch: 2 | loss: 4.3784766\n",
            "\tspeed: 0.4206s/iter; left time: 2184.9334s\n",
            "\titers: 440, epoch: 2 | loss: 6.3435578\n",
            "\tspeed: 0.4289s/iter; left time: 2223.8469s\n",
            "\titers: 450, epoch: 2 | loss: 3.7429307\n",
            "\tspeed: 0.4227s/iter; left time: 2187.5201s\n",
            "\titers: 460, epoch: 2 | loss: 4.2127986\n",
            "\tspeed: 0.4217s/iter; left time: 2177.8627s\n",
            "\titers: 470, epoch: 2 | loss: 5.7135682\n",
            "\tspeed: 0.4205s/iter; left time: 2167.7543s\n",
            "\titers: 480, epoch: 2 | loss: 3.6548989\n",
            "\tspeed: 0.4251s/iter; left time: 2187.2477s\n",
            "\titers: 490, epoch: 2 | loss: 3.0463886\n",
            "\tspeed: 0.4233s/iter; left time: 2173.5712s\n",
            "\titers: 500, epoch: 2 | loss: 4.8738856\n",
            "\tspeed: 0.4210s/iter; left time: 2157.6214s\n",
            "\titers: 510, epoch: 2 | loss: 2.7189629\n",
            "\tspeed: 0.4227s/iter; left time: 2162.3149s\n",
            "\titers: 520, epoch: 2 | loss: 6.5231557\n",
            "\tspeed: 0.4192s/iter; left time: 2139.7734s\n",
            "\titers: 530, epoch: 2 | loss: 4.7553535\n",
            "\tspeed: 0.4224s/iter; left time: 2152.2196s\n",
            "\titers: 540, epoch: 2 | loss: 6.3560300\n",
            "\tspeed: 0.4208s/iter; left time: 2139.8428s\n",
            "\titers: 550, epoch: 2 | loss: 3.5395720\n",
            "\tspeed: 0.4189s/iter; left time: 2125.9272s\n",
            "\titers: 560, epoch: 2 | loss: 2.9264538\n",
            "\tspeed: 0.4207s/iter; left time: 2130.7706s\n",
            "\titers: 570, epoch: 2 | loss: 5.8995271\n",
            "\tspeed: 0.4217s/iter; left time: 2131.6529s\n",
            "\titers: 580, epoch: 2 | loss: 3.7510703\n",
            "\tspeed: 0.4259s/iter; left time: 2148.8817s\n",
            "\titers: 590, epoch: 2 | loss: 4.4089603\n",
            "\tspeed: 0.4222s/iter; left time: 2125.7344s\n",
            "\titers: 600, epoch: 2 | loss: 4.1185055\n",
            "\tspeed: 0.4214s/iter; left time: 2117.4454s\n",
            "\titers: 610, epoch: 2 | loss: 5.0936484\n",
            "\tspeed: 0.4252s/iter; left time: 2132.6055s\n",
            "\titers: 620, epoch: 2 | loss: 6.1901608\n",
            "\tspeed: 0.4225s/iter; left time: 2114.7288s\n",
            "\titers: 630, epoch: 2 | loss: 5.9161663\n",
            "\tspeed: 0.4273s/iter; left time: 2134.3037s\n",
            "\titers: 640, epoch: 2 | loss: 4.3631883\n",
            "\tspeed: 0.4216s/iter; left time: 2101.5226s\n",
            "\titers: 650, epoch: 2 | loss: 7.0523086\n",
            "\tspeed: 0.4210s/iter; left time: 2094.2269s\n",
            "\titers: 660, epoch: 2 | loss: 2.5871055\n",
            "\tspeed: 0.4234s/iter; left time: 2102.0707s\n",
            "\titers: 670, epoch: 2 | loss: 6.3415279\n",
            "\tspeed: 0.4214s/iter; left time: 2087.8962s\n",
            "\titers: 680, epoch: 2 | loss: 5.3909588\n",
            "\tspeed: 0.4271s/iter; left time: 2111.8149s\n",
            "\titers: 690, epoch: 2 | loss: 5.8277946\n",
            "\tspeed: 0.4204s/iter; left time: 2074.7838s\n",
            "\titers: 700, epoch: 2 | loss: 4.8776507\n",
            "\tspeed: 0.4210s/iter; left time: 2073.5325s\n",
            "Epoch: 2, Train Loss: 4.9646, Validation Loss: 7.9758\n",
            "Validation loss decreased (8.198812 --> 7.975780).  Saving model ...\n",
            "\titers: 10, epoch: 3 | loss: 4.6663032\n",
            "\tspeed: 0.4274s/iter; left time: 2099.4570s\n",
            "\titers: 20, epoch: 3 | loss: 3.9455945\n",
            "\tspeed: 0.4334s/iter; left time: 2124.7012s\n",
            "\titers: 30, epoch: 3 | loss: 6.0498714\n",
            "\tspeed: 0.4227s/iter; left time: 2067.9547s\n",
            "\titers: 40, epoch: 3 | loss: 5.0468783\n",
            "\tspeed: 0.4300s/iter; left time: 2099.4452s\n",
            "\titers: 50, epoch: 3 | loss: 6.0813012\n",
            "\tspeed: 0.4488s/iter; left time: 2186.5901s\n",
            "\titers: 60, epoch: 3 | loss: 4.4307113\n",
            "\tspeed: 0.4372s/iter; left time: 2125.6835s\n",
            "\titers: 70, epoch: 3 | loss: 5.0841150\n",
            "\tspeed: 0.4283s/iter; left time: 2078.3106s\n",
            "\titers: 80, epoch: 3 | loss: 5.8127441\n",
            "\tspeed: 0.4219s/iter; left time: 2042.9207s\n",
            "\titers: 90, epoch: 3 | loss: 5.1174726\n",
            "\tspeed: 0.4376s/iter; left time: 2114.2859s\n",
            "\titers: 100, epoch: 3 | loss: 4.9338593\n",
            "\tspeed: 0.4204s/iter; left time: 2027.2362s\n",
            "\titers: 110, epoch: 3 | loss: 8.4336443\n",
            "\tspeed: 0.4180s/iter; left time: 2011.1963s\n",
            "\titers: 120, epoch: 3 | loss: 8.6903267\n",
            "\tspeed: 0.4237s/iter; left time: 2034.6129s\n",
            "\titers: 130, epoch: 3 | loss: 4.7566838\n",
            "\tspeed: 0.4276s/iter; left time: 2048.8239s\n",
            "\titers: 140, epoch: 3 | loss: 5.7467041\n",
            "\tspeed: 0.4178s/iter; left time: 1997.8075s\n",
            "\titers: 150, epoch: 3 | loss: 2.0923588\n",
            "\tspeed: 0.4186s/iter; left time: 1997.6783s\n",
            "\titers: 160, epoch: 3 | loss: 5.0823045\n",
            "\tspeed: 0.4185s/iter; left time: 1992.7688s\n",
            "\titers: 170, epoch: 3 | loss: 7.1702099\n",
            "\tspeed: 0.4174s/iter; left time: 1983.6045s\n",
            "\titers: 180, epoch: 3 | loss: 4.3026838\n",
            "\tspeed: 0.4304s/iter; left time: 2040.8627s\n",
            "\titers: 190, epoch: 3 | loss: 4.0911179\n",
            "\tspeed: 0.4272s/iter; left time: 2021.4960s\n",
            "\titers: 200, epoch: 3 | loss: 5.3574319\n",
            "\tspeed: 0.4460s/iter; left time: 2105.8565s\n",
            "\titers: 210, epoch: 3 | loss: 3.6782677\n",
            "\tspeed: 0.4457s/iter; left time: 2100.3335s\n",
            "\titers: 220, epoch: 3 | loss: 3.9741983\n",
            "\tspeed: 0.4491s/iter; left time: 2111.5952s\n",
            "\titers: 230, epoch: 3 | loss: 7.4635644\n",
            "\tspeed: 0.4408s/iter; left time: 2068.1547s\n",
            "\titers: 240, epoch: 3 | loss: 5.6091123\n",
            "\tspeed: 0.4403s/iter; left time: 2061.6059s\n",
            "\titers: 250, epoch: 3 | loss: 5.2570496\n",
            "\tspeed: 0.4394s/iter; left time: 2052.7478s\n",
            "\titers: 260, epoch: 3 | loss: 6.8746009\n",
            "\tspeed: 0.4391s/iter; left time: 2047.1313s\n",
            "\titers: 270, epoch: 3 | loss: 5.6221366\n",
            "\tspeed: 0.4426s/iter; left time: 2059.0973s\n",
            "\titers: 280, epoch: 3 | loss: 6.5321174\n",
            "\tspeed: 0.4513s/iter; left time: 2094.7670s\n",
            "\titers: 290, epoch: 3 | loss: 5.0192695\n",
            "\tspeed: 0.4289s/iter; left time: 1986.6307s\n",
            "\titers: 300, epoch: 3 | loss: 5.0030327\n",
            "\tspeed: 0.4225s/iter; left time: 1952.9072s\n",
            "\titers: 310, epoch: 3 | loss: 3.4982102\n",
            "\tspeed: 0.4218s/iter; left time: 1945.1538s\n",
            "\titers: 320, epoch: 3 | loss: 5.8650131\n",
            "\tspeed: 0.4387s/iter; left time: 2018.9432s\n",
            "\titers: 330, epoch: 3 | loss: 4.9608254\n",
            "\tspeed: 0.4234s/iter; left time: 1944.2014s\n",
            "\titers: 340, epoch: 3 | loss: 4.5525546\n",
            "\tspeed: 0.4206s/iter; left time: 1927.4031s\n",
            "\titers: 350, epoch: 3 | loss: 3.7480412\n",
            "\tspeed: 0.4188s/iter; left time: 1914.6324s\n",
            "\titers: 360, epoch: 3 | loss: 4.8113837\n",
            "\tspeed: 0.4220s/iter; left time: 1925.0222s\n",
            "\titers: 370, epoch: 3 | loss: 4.8910060\n",
            "\tspeed: 0.4194s/iter; left time: 1909.2382s\n",
            "\titers: 380, epoch: 3 | loss: 7.0599155\n",
            "\tspeed: 0.4206s/iter; left time: 1910.1478s\n",
            "\titers: 390, epoch: 3 | loss: 7.2305198\n",
            "\tspeed: 0.4229s/iter; left time: 1916.6685s\n",
            "\titers: 400, epoch: 3 | loss: 4.4735413\n",
            "\tspeed: 0.4262s/iter; left time: 1927.3459s\n",
            "\titers: 410, epoch: 3 | loss: 3.5166922\n",
            "\tspeed: 0.4215s/iter; left time: 1901.7678s\n",
            "\titers: 420, epoch: 3 | loss: 4.3452544\n",
            "\tspeed: 0.4212s/iter; left time: 1896.3186s\n",
            "\titers: 430, epoch: 3 | loss: 6.7736154\n",
            "\tspeed: 0.4203s/iter; left time: 1887.8271s\n",
            "\titers: 440, epoch: 3 | loss: 4.3406529\n",
            "\tspeed: 0.4192s/iter; left time: 1878.8784s\n",
            "\titers: 450, epoch: 3 | loss: 3.3756161\n",
            "\tspeed: 0.4194s/iter; left time: 1875.5926s\n",
            "\titers: 460, epoch: 3 | loss: 5.8527641\n",
            "\tspeed: 0.4206s/iter; left time: 1876.8111s\n",
            "\titers: 470, epoch: 3 | loss: 3.8615558\n",
            "\tspeed: 0.4189s/iter; left time: 1864.7906s\n",
            "\titers: 480, epoch: 3 | loss: 2.9184234\n",
            "\tspeed: 0.4203s/iter; left time: 1866.7729s\n",
            "\titers: 490, epoch: 3 | loss: 3.5262163\n",
            "\tspeed: 0.4210s/iter; left time: 1866.0445s\n",
            "\titers: 500, epoch: 3 | loss: 3.3078251\n",
            "\tspeed: 0.4214s/iter; left time: 1863.5681s\n",
            "\titers: 510, epoch: 3 | loss: 6.1834702\n",
            "\tspeed: 0.4191s/iter; left time: 1848.9723s\n",
            "\titers: 520, epoch: 3 | loss: 6.8812981\n",
            "\tspeed: 0.4212s/iter; left time: 1854.1757s\n",
            "\titers: 530, epoch: 3 | loss: 4.1269851\n",
            "\tspeed: 0.4196s/iter; left time: 1842.7013s\n",
            "\titers: 540, epoch: 3 | loss: 4.8925943\n",
            "\tspeed: 0.4198s/iter; left time: 1839.6247s\n",
            "\titers: 550, epoch: 3 | loss: 4.3553929\n",
            "\tspeed: 0.4206s/iter; left time: 1838.9751s\n",
            "\titers: 560, epoch: 3 | loss: 3.3124142\n",
            "\tspeed: 0.4207s/iter; left time: 1835.0361s\n",
            "\titers: 570, epoch: 3 | loss: 5.9770904\n",
            "\tspeed: 0.4211s/iter; left time: 1832.5364s\n",
            "\titers: 580, epoch: 3 | loss: 4.9541659\n",
            "\tspeed: 0.4297s/iter; left time: 1865.7713s\n",
            "\titers: 590, epoch: 3 | loss: 5.1787229\n",
            "\tspeed: 0.4246s/iter; left time: 1839.5077s\n",
            "\titers: 600, epoch: 3 | loss: 5.1896515\n",
            "\tspeed: 0.4183s/iter; left time: 1807.6898s\n",
            "\titers: 610, epoch: 3 | loss: 4.2310386\n",
            "\tspeed: 0.4207s/iter; left time: 1814.0458s\n",
            "\titers: 620, epoch: 3 | loss: 4.0529647\n",
            "\tspeed: 0.4243s/iter; left time: 1825.4172s\n",
            "\titers: 630, epoch: 3 | loss: 7.2772083\n",
            "\tspeed: 0.4302s/iter; left time: 1846.3526s\n",
            "\titers: 640, epoch: 3 | loss: 5.5542259\n",
            "\tspeed: 0.4218s/iter; left time: 1806.0857s\n",
            "\titers: 650, epoch: 3 | loss: 3.2969882\n",
            "\tspeed: 0.4204s/iter; left time: 1796.0004s\n",
            "\titers: 660, epoch: 3 | loss: 4.0414519\n",
            "\tspeed: 0.4281s/iter; left time: 1824.3841s\n",
            "\titers: 670, epoch: 3 | loss: 7.7025733\n",
            "\tspeed: 0.4232s/iter; left time: 1799.3531s\n",
            "\titers: 680, epoch: 3 | loss: 4.7345443\n",
            "\tspeed: 0.4229s/iter; left time: 1793.9233s\n",
            "\titers: 690, epoch: 3 | loss: 3.2978165\n",
            "\tspeed: 0.4254s/iter; left time: 1800.4943s\n",
            "\titers: 700, epoch: 3 | loss: 6.3966403\n",
            "\tspeed: 0.4243s/iter; left time: 1791.5672s\n",
            "Epoch: 3, Train Loss: 4.8509, Validation Loss: 7.6778\n",
            "Validation loss decreased (7.975780 --> 7.677822).  Saving model ...\n",
            "\titers: 10, epoch: 4 | loss: 4.7210608\n",
            "\tspeed: 0.4227s/iter; left time: 1779.2375s\n",
            "\titers: 20, epoch: 4 | loss: 3.1569993\n",
            "\tspeed: 0.4266s/iter; left time: 1791.2501s\n",
            "\titers: 30, epoch: 4 | loss: 4.4957509\n",
            "\tspeed: 0.4261s/iter; left time: 1784.9533s\n",
            "\titers: 40, epoch: 4 | loss: 4.2477822\n",
            "\tspeed: 0.4467s/iter; left time: 1866.6733s\n",
            "\titers: 50, epoch: 4 | loss: 3.3571732\n",
            "\tspeed: 0.4551s/iter; left time: 1897.4190s\n",
            "\titers: 60, epoch: 4 | loss: 2.8783886\n",
            "\tspeed: 0.4420s/iter; left time: 1838.4631s\n",
            "\titers: 70, epoch: 4 | loss: 6.8731360\n",
            "\tspeed: 0.4259s/iter; left time: 1767.0149s\n",
            "\titers: 80, epoch: 4 | loss: 6.5790710\n",
            "\tspeed: 0.4235s/iter; left time: 1752.6762s\n",
            "\titers: 90, epoch: 4 | loss: 4.6798301\n",
            "\tspeed: 0.4236s/iter; left time: 1749.2307s\n",
            "\titers: 100, epoch: 4 | loss: 6.8027911\n",
            "\tspeed: 0.4216s/iter; left time: 1736.6477s\n",
            "\titers: 110, epoch: 4 | loss: 4.1272597\n",
            "\tspeed: 0.4231s/iter; left time: 1738.4439s\n",
            "\titers: 120, epoch: 4 | loss: 5.0544629\n",
            "\tspeed: 0.4232s/iter; left time: 1734.7557s\n",
            "\titers: 130, epoch: 4 | loss: 5.3020415\n",
            "\tspeed: 0.4234s/iter; left time: 1731.3197s\n",
            "\titers: 140, epoch: 4 | loss: 3.7133152\n",
            "\tspeed: 0.4238s/iter; left time: 1728.6418s\n",
            "\titers: 150, epoch: 4 | loss: 5.3468704\n",
            "\tspeed: 0.4249s/iter; left time: 1728.9757s\n",
            "\titers: 160, epoch: 4 | loss: 4.5758548\n",
            "\tspeed: 0.4234s/iter; left time: 1718.6118s\n",
            "\titers: 170, epoch: 4 | loss: 3.6532974\n",
            "\tspeed: 0.4289s/iter; left time: 1736.7888s\n",
            "\titers: 180, epoch: 4 | loss: 8.4422483\n",
            "\tspeed: 0.4495s/iter; left time: 1815.7034s\n",
            "\titers: 190, epoch: 4 | loss: 5.0683360\n",
            "\tspeed: 0.4360s/iter; left time: 1756.4478s\n",
            "\titers: 200, epoch: 4 | loss: 5.9341321\n",
            "\tspeed: 0.4343s/iter; left time: 1745.4293s\n",
            "\titers: 210, epoch: 4 | loss: 5.1989474\n",
            "\tspeed: 0.4456s/iter; left time: 1786.4787s\n",
            "\titers: 220, epoch: 4 | loss: 5.3053403\n",
            "\tspeed: 0.4348s/iter; left time: 1738.8078s\n",
            "\titers: 230, epoch: 4 | loss: 5.4926376\n",
            "\tspeed: 0.4352s/iter; left time: 1736.0795s\n",
            "\titers: 240, epoch: 4 | loss: 7.8564487\n",
            "\tspeed: 0.4360s/iter; left time: 1734.9414s\n",
            "\titers: 250, epoch: 4 | loss: 4.5394816\n",
            "\tspeed: 0.4300s/iter; left time: 1706.5265s\n",
            "\titers: 260, epoch: 4 | loss: 4.4527135\n",
            "\tspeed: 0.4307s/iter; left time: 1705.2826s\n",
            "\titers: 270, epoch: 4 | loss: 3.1528661\n",
            "\tspeed: 0.4289s/iter; left time: 1693.9042s\n",
            "\titers: 280, epoch: 4 | loss: 4.7431874\n",
            "\tspeed: 0.4336s/iter; left time: 1707.8279s\n",
            "\titers: 290, epoch: 4 | loss: 4.0044990\n",
            "\tspeed: 0.4422s/iter; left time: 1737.5089s\n",
            "\titers: 300, epoch: 4 | loss: 5.9239612\n",
            "\tspeed: 0.4269s/iter; left time: 1673.0671s\n",
            "\titers: 310, epoch: 4 | loss: 5.0455565\n",
            "\tspeed: 0.4352s/iter; left time: 1701.0981s\n",
            "\titers: 320, epoch: 4 | loss: 3.4388840\n",
            "\tspeed: 0.4281s/iter; left time: 1669.1452s\n",
            "\titers: 330, epoch: 4 | loss: 4.9796462\n",
            "\tspeed: 0.4281s/iter; left time: 1664.8863s\n",
            "\titers: 340, epoch: 4 | loss: 5.4919248\n",
            "\tspeed: 0.4308s/iter; left time: 1670.9699s\n",
            "\titers: 350, epoch: 4 | loss: 7.5436058\n",
            "\tspeed: 0.4263s/iter; left time: 1649.1618s\n",
            "\titers: 360, epoch: 4 | loss: 5.4687877\n",
            "\tspeed: 0.4229s/iter; left time: 1631.8301s\n",
            "\titers: 370, epoch: 4 | loss: 3.5504658\n",
            "\tspeed: 0.4226s/iter; left time: 1626.5188s\n",
            "\titers: 380, epoch: 4 | loss: 4.7785296\n",
            "\tspeed: 0.4238s/iter; left time: 1627.0617s\n",
            "\titers: 390, epoch: 4 | loss: 3.3823395\n",
            "\tspeed: 0.4244s/iter; left time: 1625.1821s\n",
            "\titers: 400, epoch: 4 | loss: 4.1396918\n",
            "\tspeed: 0.4236s/iter; left time: 1617.6724s\n",
            "\titers: 410, epoch: 4 | loss: 4.1094799\n",
            "\tspeed: 0.4242s/iter; left time: 1615.7548s\n",
            "\titers: 420, epoch: 4 | loss: 5.1151576\n",
            "\tspeed: 0.4243s/iter; left time: 1611.7723s\n",
            "\titers: 430, epoch: 4 | loss: 4.0922341\n",
            "\tspeed: 0.4253s/iter; left time: 1611.5941s\n",
            "\titers: 440, epoch: 4 | loss: 3.5873597\n",
            "\tspeed: 0.4322s/iter; left time: 1633.1484s\n",
            "\titers: 450, epoch: 4 | loss: 8.1407404\n",
            "\tspeed: 0.4235s/iter; left time: 1596.1269s\n",
            "\titers: 460, epoch: 4 | loss: 3.5642817\n",
            "\tspeed: 0.4232s/iter; left time: 1590.7634s\n",
            "\titers: 470, epoch: 4 | loss: 5.5477414\n",
            "\tspeed: 0.4244s/iter; left time: 1591.0041s\n",
            "\titers: 480, epoch: 4 | loss: 6.6338253\n",
            "\tspeed: 0.4231s/iter; left time: 1581.8077s\n",
            "\titers: 490, epoch: 4 | loss: 5.1809182\n",
            "\tspeed: 0.4255s/iter; left time: 1586.6991s\n",
            "\titers: 500, epoch: 4 | loss: 6.0186214\n",
            "\tspeed: 0.4229s/iter; left time: 1572.7508s\n",
            "\titers: 510, epoch: 4 | loss: 3.0808880\n",
            "\tspeed: 0.4230s/iter; left time: 1568.8272s\n",
            "\titers: 520, epoch: 4 | loss: 3.4440868\n",
            "\tspeed: 0.4199s/iter; left time: 1553.3663s\n",
            "\titers: 530, epoch: 4 | loss: 2.9978874\n",
            "\tspeed: 0.4220s/iter; left time: 1556.7520s\n",
            "\titers: 540, epoch: 4 | loss: 5.3663754\n",
            "\tspeed: 0.4385s/iter; left time: 1613.0676s\n",
            "\titers: 550, epoch: 4 | loss: 3.7727280\n",
            "\tspeed: 0.4382s/iter; left time: 1607.6645s\n",
            "\titers: 560, epoch: 4 | loss: 4.2773385\n",
            "\tspeed: 0.4328s/iter; left time: 1583.4487s\n",
            "\titers: 570, epoch: 4 | loss: 4.1873226\n",
            "\tspeed: 0.4308s/iter; left time: 1571.9137s\n",
            "\titers: 580, epoch: 4 | loss: 4.1404786\n",
            "\tspeed: 0.4231s/iter; left time: 1539.5009s\n",
            "\titers: 590, epoch: 4 | loss: 3.4662540\n",
            "\tspeed: 0.4236s/iter; left time: 1537.0647s\n",
            "\titers: 600, epoch: 4 | loss: 4.7109332\n",
            "\tspeed: 0.4232s/iter; left time: 1531.5457s\n",
            "\titers: 610, epoch: 4 | loss: 4.7372870\n",
            "\tspeed: 0.4243s/iter; left time: 1531.3828s\n",
            "\titers: 620, epoch: 4 | loss: 4.3797150\n",
            "\tspeed: 0.4237s/iter; left time: 1525.0313s\n",
            "\titers: 630, epoch: 4 | loss: 5.5694184\n",
            "\tspeed: 0.4243s/iter; left time: 1522.7631s\n",
            "\titers: 640, epoch: 4 | loss: 4.4554086\n",
            "\tspeed: 0.4237s/iter; left time: 1516.4000s\n",
            "\titers: 650, epoch: 4 | loss: 4.4188905\n",
            "\tspeed: 0.4236s/iter; left time: 1511.8142s\n",
            "\titers: 660, epoch: 4 | loss: 3.5590513\n",
            "\tspeed: 0.4230s/iter; left time: 1505.2792s\n",
            "\titers: 670, epoch: 4 | loss: 4.0552592\n",
            "\tspeed: 0.4243s/iter; left time: 1505.7866s\n",
            "\titers: 680, epoch: 4 | loss: 5.4584870\n",
            "\tspeed: 0.4339s/iter; left time: 1535.4319s\n",
            "\titers: 690, epoch: 4 | loss: 3.4596584\n",
            "\tspeed: 0.4384s/iter; left time: 1547.1305s\n",
            "\titers: 700, epoch: 4 | loss: 3.7319195\n",
            "\tspeed: 0.4524s/iter; left time: 1592.1199s\n",
            "Epoch: 4, Train Loss: 4.6898, Validation Loss: 7.4784\n",
            "Validation loss decreased (7.677822 --> 7.478436).  Saving model ...\n",
            "\titers: 10, epoch: 5 | loss: 4.9595037\n",
            "\tspeed: 0.4245s/iter; left time: 1488.4366s\n",
            "\titers: 20, epoch: 5 | loss: 5.4828854\n",
            "\tspeed: 0.4194s/iter; left time: 1466.3275s\n",
            "\titers: 30, epoch: 5 | loss: 3.3356018\n",
            "\tspeed: 0.4582s/iter; left time: 1597.3618s\n",
            "\titers: 40, epoch: 5 | loss: 4.1480179\n",
            "\tspeed: 0.4427s/iter; left time: 1538.9001s\n",
            "\titers: 50, epoch: 5 | loss: 4.3757796\n",
            "\tspeed: 0.4353s/iter; left time: 1508.8819s\n",
            "\titers: 60, epoch: 5 | loss: 4.1816897\n",
            "\tspeed: 0.4546s/iter; left time: 1571.0811s\n",
            "\titers: 70, epoch: 5 | loss: 4.8049698\n",
            "\tspeed: 0.4559s/iter; left time: 1571.0006s\n",
            "\titers: 80, epoch: 5 | loss: 7.7591248\n",
            "\tspeed: 0.4527s/iter; left time: 1555.4561s\n",
            "\titers: 90, epoch: 5 | loss: 3.6410291\n",
            "\tspeed: 0.4690s/iter; left time: 1606.7885s\n",
            "\titers: 100, epoch: 5 | loss: 5.4447346\n",
            "\tspeed: 0.4518s/iter; left time: 1543.4518s\n",
            "\titers: 110, epoch: 5 | loss: 3.8558607\n",
            "\tspeed: 0.4404s/iter; left time: 1500.0774s\n",
            "\titers: 120, epoch: 5 | loss: 6.4865117\n",
            "\tspeed: 0.4339s/iter; left time: 1473.6920s\n",
            "\titers: 130, epoch: 5 | loss: 4.7721901\n",
            "\tspeed: 0.4376s/iter; left time: 1481.7551s\n",
            "\titers: 140, epoch: 5 | loss: 5.9084907\n",
            "\tspeed: 0.4397s/iter; left time: 1484.4924s\n",
            "\titers: 150, epoch: 5 | loss: 5.7722783\n",
            "\tspeed: 0.4493s/iter; left time: 1512.4766s\n",
            "\titers: 160, epoch: 5 | loss: 3.6690598\n",
            "\tspeed: 0.4542s/iter; left time: 1524.2395s\n",
            "\titers: 170, epoch: 5 | loss: 3.9093237\n",
            "\tspeed: 0.4598s/iter; left time: 1538.6155s\n",
            "\titers: 180, epoch: 5 | loss: 4.4986959\n",
            "\tspeed: 0.4501s/iter; left time: 1501.5883s\n",
            "\titers: 190, epoch: 5 | loss: 5.6595569\n",
            "\tspeed: 0.4558s/iter; left time: 1516.1190s\n",
            "\titers: 200, epoch: 5 | loss: 4.2548232\n",
            "\tspeed: 0.4465s/iter; left time: 1480.6458s\n",
            "\titers: 210, epoch: 5 | loss: 6.1603770\n",
            "\tspeed: 0.4406s/iter; left time: 1456.6426s\n",
            "\titers: 220, epoch: 5 | loss: 3.6738098\n",
            "\tspeed: 0.4356s/iter; left time: 1435.7433s\n",
            "\titers: 230, epoch: 5 | loss: 4.6870093\n",
            "\tspeed: 0.4496s/iter; left time: 1477.3629s\n",
            "\titers: 240, epoch: 5 | loss: 4.8177571\n",
            "\tspeed: 0.4346s/iter; left time: 1423.6166s\n",
            "\titers: 250, epoch: 5 | loss: 4.6374798\n",
            "\tspeed: 0.4355s/iter; left time: 1422.4165s\n",
            "\titers: 260, epoch: 5 | loss: 3.6972716\n",
            "\tspeed: 0.4384s/iter; left time: 1427.4664s\n",
            "\titers: 270, epoch: 5 | loss: 4.0423493\n",
            "\tspeed: 0.4322s/iter; left time: 1402.9506s\n",
            "\titers: 280, epoch: 5 | loss: 3.5743320\n",
            "\tspeed: 0.4324s/iter; left time: 1399.2571s\n",
            "\titers: 290, epoch: 5 | loss: 8.3427591\n",
            "\tspeed: 0.4309s/iter; left time: 1390.0885s\n",
            "\titers: 300, epoch: 5 | loss: 4.2666798\n",
            "\tspeed: 0.4325s/iter; left time: 1390.8007s\n",
            "\titers: 310, epoch: 5 | loss: 6.5609059\n",
            "\tspeed: 0.4309s/iter; left time: 1381.6134s\n",
            "\titers: 320, epoch: 5 | loss: 3.6208909\n",
            "\tspeed: 0.4437s/iter; left time: 1418.0356s\n",
            "\titers: 330, epoch: 5 | loss: 3.0747654\n",
            "\tspeed: 0.4496s/iter; left time: 1432.2899s\n",
            "\titers: 340, epoch: 5 | loss: 2.8254621\n",
            "\tspeed: 0.4304s/iter; left time: 1366.9131s\n",
            "\titers: 350, epoch: 5 | loss: 4.9285350\n",
            "\tspeed: 0.4233s/iter; left time: 1340.3082s\n",
            "\titers: 360, epoch: 5 | loss: 5.9339051\n",
            "\tspeed: 0.4396s/iter; left time: 1387.2799s\n",
            "\titers: 370, epoch: 5 | loss: 2.3922863\n",
            "\tspeed: 0.4297s/iter; left time: 1351.8125s\n",
            "\titers: 380, epoch: 5 | loss: 3.8755944\n",
            "\tspeed: 0.4384s/iter; left time: 1374.8401s\n",
            "\titers: 390, epoch: 5 | loss: 4.1885562\n",
            "\tspeed: 0.4423s/iter; left time: 1382.6430s\n",
            "\titers: 400, epoch: 5 | loss: 4.9248128\n",
            "\tspeed: 0.4442s/iter; left time: 1384.0880s\n",
            "\titers: 410, epoch: 5 | loss: 3.3346386\n",
            "\tspeed: 0.4445s/iter; left time: 1380.5999s\n",
            "\titers: 420, epoch: 5 | loss: 5.1553030\n",
            "\tspeed: 0.4408s/iter; left time: 1364.7066s\n",
            "\titers: 430, epoch: 5 | loss: 6.3087707\n",
            "\tspeed: 0.4415s/iter; left time: 1362.4064s\n",
            "\titers: 440, epoch: 5 | loss: 3.3271933\n",
            "\tspeed: 0.4256s/iter; left time: 1309.1643s\n",
            "\titers: 450, epoch: 5 | loss: 4.2779183\n",
            "\tspeed: 0.4230s/iter; left time: 1297.0226s\n",
            "\titers: 460, epoch: 5 | loss: 4.9678464\n",
            "\tspeed: 0.4239s/iter; left time: 1295.4110s\n",
            "\titers: 470, epoch: 5 | loss: 3.4186180\n",
            "\tspeed: 0.4227s/iter; left time: 1287.5262s\n",
            "\titers: 480, epoch: 5 | loss: 3.9611061\n",
            "\tspeed: 0.4227s/iter; left time: 1283.2977s\n",
            "\titers: 490, epoch: 5 | loss: 3.5526676\n",
            "\tspeed: 0.4228s/iter; left time: 1279.2717s\n",
            "\titers: 500, epoch: 5 | loss: 4.2936683\n",
            "\tspeed: 0.4221s/iter; left time: 1273.0277s\n",
            "\titers: 510, epoch: 5 | loss: 3.8241568\n",
            "\tspeed: 0.4224s/iter; left time: 1269.7552s\n",
            "\titers: 520, epoch: 5 | loss: 4.7977481\n",
            "\tspeed: 0.4219s/iter; left time: 1264.0420s\n",
            "\titers: 530, epoch: 5 | loss: 4.3504357\n",
            "\tspeed: 0.4222s/iter; left time: 1260.7187s\n",
            "\titers: 540, epoch: 5 | loss: 3.1387875\n",
            "\tspeed: 0.4222s/iter; left time: 1256.5119s\n",
            "\titers: 550, epoch: 5 | loss: 3.9691727\n",
            "\tspeed: 0.4230s/iter; left time: 1254.6369s\n",
            "\titers: 560, epoch: 5 | loss: 3.6024182\n",
            "\tspeed: 0.4278s/iter; left time: 1264.5690s\n",
            "\titers: 570, epoch: 5 | loss: 3.0741565\n",
            "\tspeed: 0.4314s/iter; left time: 1270.9627s\n",
            "\titers: 580, epoch: 5 | loss: 3.1032131\n",
            "\tspeed: 0.4294s/iter; left time: 1260.7970s\n",
            "\titers: 590, epoch: 5 | loss: 3.0808773\n",
            "\tspeed: 0.4310s/iter; left time: 1260.9826s\n",
            "\titers: 600, epoch: 5 | loss: 4.4201818\n",
            "\tspeed: 0.4240s/iter; left time: 1236.3548s\n",
            "\titers: 610, epoch: 5 | loss: 4.3152356\n",
            "\tspeed: 0.4235s/iter; left time: 1230.6053s\n",
            "\titers: 620, epoch: 5 | loss: 3.1203840\n",
            "\tspeed: 0.4213s/iter; left time: 1219.9744s\n",
            "\titers: 630, epoch: 5 | loss: 4.2702990\n",
            "\tspeed: 0.4233s/iter; left time: 1221.6007s\n",
            "\titers: 640, epoch: 5 | loss: 3.7981865\n",
            "\tspeed: 0.4244s/iter; left time: 1220.4465s\n",
            "\titers: 650, epoch: 5 | loss: 4.3132253\n",
            "\tspeed: 0.4230s/iter; left time: 1212.2762s\n",
            "\titers: 660, epoch: 5 | loss: 5.2172108\n",
            "\tspeed: 0.4227s/iter; left time: 1207.1045s\n",
            "\titers: 670, epoch: 5 | loss: 5.7512631\n",
            "\tspeed: 0.4228s/iter; left time: 1203.2224s\n",
            "\titers: 680, epoch: 5 | loss: 2.9211540\n",
            "\tspeed: 0.4199s/iter; left time: 1190.7487s\n",
            "\titers: 690, epoch: 5 | loss: 3.0995791\n",
            "\tspeed: 0.4236s/iter; left time: 1197.1846s\n",
            "\titers: 700, epoch: 5 | loss: 7.1568990\n",
            "\tspeed: 0.4222s/iter; left time: 1188.7949s\n",
            "Epoch: 5, Train Loss: 4.4823, Validation Loss: 7.0791\n",
            "Validation loss decreased (7.478436 --> 7.079054).  Saving model ...\n",
            "\titers: 10, epoch: 6 | loss: 4.5503154\n",
            "\tspeed: 0.4212s/iter; left time: 1180.6272s\n",
            "\titers: 20, epoch: 6 | loss: 4.0519295\n",
            "\tspeed: 0.4243s/iter; left time: 1185.0525s\n",
            "\titers: 30, epoch: 6 | loss: 2.5089242\n",
            "\tspeed: 0.4297s/iter; left time: 1195.8319s\n",
            "\titers: 40, epoch: 6 | loss: 4.0254445\n",
            "\tspeed: 0.4203s/iter; left time: 1165.5829s\n",
            "\titers: 50, epoch: 6 | loss: 3.1947660\n",
            "\tspeed: 0.4211s/iter; left time: 1163.4433s\n",
            "\titers: 60, epoch: 6 | loss: 4.6338954\n",
            "\tspeed: 0.4222s/iter; left time: 1162.2692s\n",
            "\titers: 70, epoch: 6 | loss: 3.0625880\n",
            "\tspeed: 0.4223s/iter; left time: 1158.4581s\n",
            "\titers: 80, epoch: 6 | loss: 2.4750175\n",
            "\tspeed: 0.4234s/iter; left time: 1157.1196s\n",
            "\titers: 90, epoch: 6 | loss: 4.1790400\n",
            "\tspeed: 0.4218s/iter; left time: 1148.5501s\n",
            "\titers: 100, epoch: 6 | loss: 5.5786915\n",
            "\tspeed: 0.4217s/iter; left time: 1144.0451s\n",
            "\titers: 110, epoch: 6 | loss: 5.1988006\n",
            "\tspeed: 0.4226s/iter; left time: 1142.3511s\n",
            "\titers: 120, epoch: 6 | loss: 4.0509672\n",
            "\tspeed: 0.4233s/iter; left time: 1139.8858s\n",
            "\titers: 130, epoch: 6 | loss: 5.5913100\n",
            "\tspeed: 0.4228s/iter; left time: 1134.3675s\n",
            "\titers: 140, epoch: 6 | loss: 3.5603335\n",
            "\tspeed: 0.4237s/iter; left time: 1132.5589s\n",
            "\titers: 150, epoch: 6 | loss: 3.1454220\n",
            "\tspeed: 0.4205s/iter; left time: 1119.8303s\n",
            "\titers: 160, epoch: 6 | loss: 5.2948003\n",
            "\tspeed: 0.4218s/iter; left time: 1118.9558s\n",
            "\titers: 170, epoch: 6 | loss: 3.8131392\n",
            "\tspeed: 0.4443s/iter; left time: 1174.3109s\n",
            "\titers: 180, epoch: 6 | loss: 4.4714203\n",
            "\tspeed: 0.4285s/iter; left time: 1128.1580s\n",
            "\titers: 190, epoch: 6 | loss: 4.8810716\n",
            "\tspeed: 0.4353s/iter; left time: 1141.7640s\n",
            "\titers: 200, epoch: 6 | loss: 7.5374055\n",
            "\tspeed: 0.4305s/iter; left time: 1124.8873s\n",
            "\titers: 210, epoch: 6 | loss: 4.8456931\n",
            "\tspeed: 0.4216s/iter; left time: 1097.4445s\n",
            "\titers: 220, epoch: 6 | loss: 6.0979829\n",
            "\tspeed: 0.4233s/iter; left time: 1097.6547s\n",
            "\titers: 230, epoch: 6 | loss: 3.6326406\n",
            "\tspeed: 0.4214s/iter; left time: 1088.5013s\n",
            "\titers: 240, epoch: 6 | loss: 2.9486501\n",
            "\tspeed: 0.4248s/iter; left time: 1092.9150s\n",
            "\titers: 250, epoch: 6 | loss: 3.8594589\n",
            "\tspeed: 0.4222s/iter; left time: 1082.1278s\n",
            "\titers: 260, epoch: 6 | loss: 3.8089616\n",
            "\tspeed: 0.4238s/iter; left time: 1081.9815s\n",
            "\titers: 270, epoch: 6 | loss: 2.8983181\n",
            "\tspeed: 0.4245s/iter; left time: 1079.4551s\n",
            "\titers: 280, epoch: 6 | loss: 3.0246232\n",
            "\tspeed: 0.4213s/iter; left time: 1067.2238s\n",
            "\titers: 290, epoch: 6 | loss: 3.5305758\n",
            "\tspeed: 0.4238s/iter; left time: 1069.3418s\n",
            "\titers: 300, epoch: 6 | loss: 2.6340597\n",
            "\tspeed: 0.4219s/iter; left time: 1060.1346s\n",
            "\titers: 310, epoch: 6 | loss: 3.3903770\n",
            "\tspeed: 0.4221s/iter; left time: 1056.5295s\n",
            "\titers: 320, epoch: 6 | loss: 2.9860513\n",
            "\tspeed: 0.4231s/iter; left time: 1054.7094s\n",
            "\titers: 330, epoch: 6 | loss: 6.6073089\n",
            "\tspeed: 0.4222s/iter; left time: 1048.3458s\n",
            "\titers: 340, epoch: 6 | loss: 3.0345428\n",
            "\tspeed: 0.4252s/iter; left time: 1051.5091s\n",
            "\titers: 350, epoch: 6 | loss: 5.2409282\n",
            "\tspeed: 0.4377s/iter; left time: 1078.0548s\n",
            "\titers: 360, epoch: 6 | loss: 4.4342899\n",
            "\tspeed: 0.4396s/iter; left time: 1078.3527s\n",
            "\titers: 370, epoch: 6 | loss: 2.2066777\n",
            "\tspeed: 0.4280s/iter; left time: 1045.5572s\n",
            "\titers: 380, epoch: 6 | loss: 3.2389488\n",
            "\tspeed: 0.4278s/iter; left time: 1040.7983s\n",
            "\titers: 390, epoch: 6 | loss: 4.4655242\n",
            "\tspeed: 0.4247s/iter; left time: 1029.1101s\n",
            "\titers: 400, epoch: 6 | loss: 2.9679267\n",
            "\tspeed: 0.4236s/iter; left time: 1022.2182s\n",
            "\titers: 410, epoch: 6 | loss: 2.4573658\n",
            "\tspeed: 0.4230s/iter; left time: 1016.3667s\n",
            "\titers: 420, epoch: 6 | loss: 3.9931574\n",
            "\tspeed: 0.4222s/iter; left time: 1010.3801s\n",
            "\titers: 430, epoch: 6 | loss: 3.3865747\n",
            "\tspeed: 0.4225s/iter; left time: 1006.8182s\n",
            "\titers: 440, epoch: 6 | loss: 2.8206768\n",
            "\tspeed: 0.4219s/iter; left time: 1001.1566s\n",
            "\titers: 450, epoch: 6 | loss: 4.7069817\n",
            "\tspeed: 0.4221s/iter; left time: 997.3761s\n",
            "\titers: 460, epoch: 6 | loss: 4.6018863\n",
            "\tspeed: 0.4228s/iter; left time: 994.8946s\n",
            "\titers: 470, epoch: 6 | loss: 2.5740886\n",
            "\tspeed: 0.4226s/iter; left time: 990.1175s\n",
            "\titers: 480, epoch: 6 | loss: 3.8165796\n",
            "\tspeed: 0.4223s/iter; left time: 985.1670s\n",
            "\titers: 490, epoch: 6 | loss: 3.9207370\n",
            "\tspeed: 0.4231s/iter; left time: 982.9198s\n",
            "\titers: 500, epoch: 6 | loss: 5.1541076\n",
            "\tspeed: 0.4255s/iter; left time: 984.1845s\n",
            "\titers: 510, epoch: 6 | loss: 4.4553123\n",
            "\tspeed: 0.4201s/iter; left time: 967.5801s\n",
            "\titers: 520, epoch: 6 | loss: 6.6459441\n",
            "\tspeed: 0.4269s/iter; left time: 978.8635s\n",
            "\titers: 530, epoch: 6 | loss: 3.2617102\n",
            "\tspeed: 0.4260s/iter; left time: 972.4597s\n",
            "\titers: 540, epoch: 6 | loss: 2.8378923\n",
            "\tspeed: 0.4204s/iter; left time: 955.6056s\n",
            "\titers: 550, epoch: 6 | loss: 6.9439826\n",
            "\tspeed: 0.4234s/iter; left time: 958.0903s\n",
            "\titers: 560, epoch: 6 | loss: 3.1347859\n",
            "\tspeed: 0.4224s/iter; left time: 951.5829s\n",
            "\titers: 570, epoch: 6 | loss: 4.4287372\n",
            "\tspeed: 0.4211s/iter; left time: 944.6322s\n",
            "\titers: 580, epoch: 6 | loss: 2.7719491\n",
            "\tspeed: 0.4222s/iter; left time: 942.7151s\n",
            "\titers: 590, epoch: 6 | loss: 6.5038033\n",
            "\tspeed: 0.4248s/iter; left time: 944.3955s\n",
            "\titers: 600, epoch: 6 | loss: 2.6129568\n",
            "\tspeed: 0.4232s/iter; left time: 936.5062s\n",
            "\titers: 610, epoch: 6 | loss: 6.8595252\n",
            "\tspeed: 0.4232s/iter; left time: 932.3521s\n",
            "\titers: 620, epoch: 6 | loss: 3.0114088\n",
            "\tspeed: 0.4219s/iter; left time: 925.1801s\n",
            "\titers: 630, epoch: 6 | loss: 3.7937496\n",
            "\tspeed: 0.4230s/iter; left time: 923.3880s\n",
            "\titers: 640, epoch: 6 | loss: 2.9737203\n",
            "\tspeed: 0.4254s/iter; left time: 924.4840s\n",
            "\titers: 650, epoch: 6 | loss: 2.9280016\n",
            "\tspeed: 0.4223s/iter; left time: 913.3815s\n",
            "\titers: 660, epoch: 6 | loss: 2.3516614\n",
            "\tspeed: 0.4211s/iter; left time: 906.7085s\n",
            "\titers: 670, epoch: 6 | loss: 2.5282917\n",
            "\tspeed: 0.4222s/iter; left time: 904.6821s\n",
            "\titers: 680, epoch: 6 | loss: 2.1686385\n",
            "\tspeed: 0.4240s/iter; left time: 904.3892s\n",
            "\titers: 690, epoch: 6 | loss: 5.1369729\n",
            "\tspeed: 0.4215s/iter; left time: 894.8507s\n",
            "\titers: 700, epoch: 6 | loss: 2.7077992\n",
            "\tspeed: 0.4228s/iter; left time: 893.3375s\n",
            "Epoch: 6, Train Loss: 4.2571, Validation Loss: 6.8963\n",
            "Validation loss decreased (7.079054 --> 6.896318).  Saving model ...\n",
            "\titers: 10, epoch: 7 | loss: 3.5069249\n",
            "\tspeed: 0.4209s/iter; left time: 883.8662s\n",
            "\titers: 20, epoch: 7 | loss: 2.7577155\n",
            "\tspeed: 0.4193s/iter; left time: 876.3369s\n",
            "\titers: 30, epoch: 7 | loss: 5.5768661\n",
            "\tspeed: 0.4220s/iter; left time: 877.7651s\n",
            "\titers: 40, epoch: 7 | loss: 4.3984904\n",
            "\tspeed: 0.4212s/iter; left time: 871.9195s\n",
            "\titers: 50, epoch: 7 | loss: 3.3592002\n",
            "\tspeed: 0.4221s/iter; left time: 869.5394s\n",
            "\titers: 60, epoch: 7 | loss: 5.0157075\n",
            "\tspeed: 0.4227s/iter; left time: 866.4363s\n",
            "\titers: 70, epoch: 7 | loss: 2.9227221\n",
            "\tspeed: 0.4267s/iter; left time: 870.4457s\n",
            "\titers: 80, epoch: 7 | loss: 2.9003775\n",
            "\tspeed: 0.4288s/iter; left time: 870.4548s\n",
            "\titers: 90, epoch: 7 | loss: 2.5359557\n",
            "\tspeed: 0.4221s/iter; left time: 852.7334s\n",
            "\titers: 100, epoch: 7 | loss: 4.1621547\n",
            "\tspeed: 0.4202s/iter; left time: 844.6717s\n",
            "\titers: 110, epoch: 7 | loss: 3.1228726\n",
            "\tspeed: 0.4220s/iter; left time: 844.0431s\n",
            "\titers: 120, epoch: 7 | loss: 2.8784189\n",
            "\tspeed: 0.4219s/iter; left time: 839.5601s\n",
            "\titers: 130, epoch: 7 | loss: 4.9122858\n",
            "\tspeed: 0.4206s/iter; left time: 832.7773s\n",
            "\titers: 140, epoch: 7 | loss: 3.3243163\n",
            "\tspeed: 0.4240s/iter; left time: 835.1993s\n",
            "\titers: 150, epoch: 7 | loss: 3.2859948\n",
            "\tspeed: 0.4230s/iter; left time: 829.0187s\n",
            "\titers: 160, epoch: 7 | loss: 4.7830882\n",
            "\tspeed: 0.4216s/iter; left time: 822.1472s\n",
            "\titers: 170, epoch: 7 | loss: 3.4643009\n",
            "\tspeed: 0.4206s/iter; left time: 815.9225s\n",
            "\titers: 180, epoch: 7 | loss: 5.4348807\n",
            "\tspeed: 0.4223s/iter; left time: 814.9577s\n",
            "\titers: 190, epoch: 7 | loss: 4.9233422\n",
            "\tspeed: 0.4221s/iter; left time: 810.4825s\n",
            "\titers: 200, epoch: 7 | loss: 4.0401120\n",
            "\tspeed: 0.4243s/iter; left time: 810.4110s\n",
            "\titers: 210, epoch: 7 | loss: 5.3104038\n",
            "\tspeed: 0.4314s/iter; left time: 819.7083s\n",
            "\titers: 220, epoch: 7 | loss: 4.2902789\n",
            "\tspeed: 0.4204s/iter; left time: 794.4930s\n",
            "\titers: 230, epoch: 7 | loss: 3.1387730\n",
            "\tspeed: 0.4234s/iter; left time: 796.0085s\n",
            "\titers: 240, epoch: 7 | loss: 2.6064041\n",
            "\tspeed: 0.4242s/iter; left time: 793.3097s\n",
            "\titers: 250, epoch: 7 | loss: 4.4659781\n",
            "\tspeed: 0.4215s/iter; left time: 783.9479s\n",
            "\titers: 260, epoch: 7 | loss: 5.1205735\n",
            "\tspeed: 0.4222s/iter; left time: 781.0469s\n",
            "\titers: 270, epoch: 7 | loss: 2.8963406\n",
            "\tspeed: 0.4220s/iter; left time: 776.4926s\n",
            "\titers: 280, epoch: 7 | loss: 3.8731976\n",
            "\tspeed: 0.4232s/iter; left time: 774.4950s\n",
            "\titers: 290, epoch: 7 | loss: 6.1898665\n",
            "\tspeed: 0.4217s/iter; left time: 767.5074s\n",
            "\titers: 300, epoch: 7 | loss: 5.4783554\n",
            "\tspeed: 0.4242s/iter; left time: 767.8492s\n",
            "\titers: 310, epoch: 7 | loss: 3.8558187\n",
            "\tspeed: 0.4214s/iter; left time: 758.5412s\n",
            "\titers: 320, epoch: 7 | loss: 4.7795529\n",
            "\tspeed: 0.4241s/iter; left time: 759.1480s\n",
            "\titers: 330, epoch: 7 | loss: 3.7927732\n",
            "\tspeed: 0.4215s/iter; left time: 750.2012s\n",
            "\titers: 340, epoch: 7 | loss: 3.8263204\n",
            "\tspeed: 0.4222s/iter; left time: 747.3267s\n",
            "\titers: 350, epoch: 7 | loss: 6.1114087\n",
            "\tspeed: 0.4225s/iter; left time: 743.6249s\n",
            "\titers: 360, epoch: 7 | loss: 2.7657793\n",
            "\tspeed: 0.4225s/iter; left time: 739.3509s\n",
            "\titers: 370, epoch: 7 | loss: 5.1795354\n",
            "\tspeed: 0.4233s/iter; left time: 736.6284s\n",
            "\titers: 380, epoch: 7 | loss: 2.8313179\n",
            "\tspeed: 0.4223s/iter; left time: 730.5690s\n",
            "\titers: 390, epoch: 7 | loss: 4.2724004\n",
            "\tspeed: 0.4214s/iter; left time: 724.8905s\n",
            "\titers: 400, epoch: 7 | loss: 6.8596692\n",
            "\tspeed: 0.4226s/iter; left time: 722.6916s\n",
            "\titers: 410, epoch: 7 | loss: 1.9969950\n",
            "\tspeed: 0.4218s/iter; left time: 716.9846s\n",
            "\titers: 420, epoch: 7 | loss: 3.6728880\n",
            "\tspeed: 0.4209s/iter; left time: 711.3450s\n",
            "\titers: 430, epoch: 7 | loss: 3.9060676\n",
            "\tspeed: 0.4234s/iter; left time: 711.3751s\n",
            "\titers: 440, epoch: 7 | loss: 2.5354187\n",
            "\tspeed: 0.4225s/iter; left time: 705.5513s\n",
            "\titers: 450, epoch: 7 | loss: 3.0291703\n",
            "\tspeed: 0.4228s/iter; left time: 701.7968s\n",
            "\titers: 460, epoch: 7 | loss: 2.6066618\n",
            "\tspeed: 0.4220s/iter; left time: 696.3031s\n",
            "\titers: 470, epoch: 7 | loss: 3.1468132\n",
            "\tspeed: 0.4217s/iter; left time: 691.5333s\n",
            "\titers: 480, epoch: 7 | loss: 4.6151776\n",
            "\tspeed: 0.4221s/iter; left time: 687.9802s\n",
            "\titers: 490, epoch: 7 | loss: 3.0440199\n",
            "\tspeed: 0.4233s/iter; left time: 685.8141s\n",
            "\titers: 500, epoch: 7 | loss: 7.5953546\n",
            "\tspeed: 0.4210s/iter; left time: 677.8415s\n",
            "\titers: 510, epoch: 7 | loss: 3.2931640\n",
            "\tspeed: 0.4226s/iter; left time: 676.1439s\n",
            "\titers: 520, epoch: 7 | loss: 2.8388357\n",
            "\tspeed: 0.4220s/iter; left time: 671.0148s\n",
            "\titers: 530, epoch: 7 | loss: 5.0876708\n",
            "\tspeed: 0.4212s/iter; left time: 665.5460s\n",
            "\titers: 540, epoch: 7 | loss: 3.6427190\n",
            "\tspeed: 0.4238s/iter; left time: 665.3031s\n",
            "\titers: 550, epoch: 7 | loss: 3.0810108\n",
            "\tspeed: 0.4219s/iter; left time: 658.1893s\n",
            "\titers: 560, epoch: 7 | loss: 5.5331120\n",
            "\tspeed: 0.4221s/iter; left time: 654.1913s\n",
            "\titers: 570, epoch: 7 | loss: 3.0554082\n",
            "\tspeed: 0.4237s/iter; left time: 652.5119s\n",
            "\titers: 580, epoch: 7 | loss: 3.4972620\n",
            "\tspeed: 0.4332s/iter; left time: 662.8374s\n",
            "\titers: 590, epoch: 7 | loss: 2.7943804\n",
            "\tspeed: 0.4287s/iter; left time: 651.5786s\n",
            "\titers: 600, epoch: 7 | loss: 3.8959301\n",
            "\tspeed: 0.4202s/iter; left time: 634.4289s\n",
            "\titers: 610, epoch: 7 | loss: 4.4015665\n",
            "\tspeed: 0.4195s/iter; left time: 629.2966s\n",
            "\titers: 620, epoch: 7 | loss: 3.0152881\n",
            "\tspeed: 0.4201s/iter; left time: 625.9956s\n",
            "\titers: 630, epoch: 7 | loss: 2.9057338\n",
            "\tspeed: 0.4219s/iter; left time: 624.4071s\n",
            "\titers: 640, epoch: 7 | loss: 7.1508489\n",
            "\tspeed: 0.4230s/iter; left time: 621.8279s\n",
            "\titers: 650, epoch: 7 | loss: 2.2586973\n",
            "\tspeed: 0.4221s/iter; left time: 616.2074s\n",
            "\titers: 660, epoch: 7 | loss: 2.5770519\n",
            "\tspeed: 0.4208s/iter; left time: 610.1465s\n",
            "\titers: 670, epoch: 7 | loss: 2.4882123\n",
            "\tspeed: 0.4211s/iter; left time: 606.4321s\n",
            "\titers: 680, epoch: 7 | loss: 2.9218528\n",
            "\tspeed: 0.4222s/iter; left time: 603.7453s\n",
            "\titers: 690, epoch: 7 | loss: 2.1214359\n",
            "\tspeed: 0.4252s/iter; left time: 603.8184s\n",
            "\titers: 700, epoch: 7 | loss: 3.6534040\n",
            "\tspeed: 0.4218s/iter; left time: 594.7063s\n",
            "Epoch: 7, Train Loss: 4.0478, Validation Loss: 6.5274\n",
            "Validation loss decreased (6.896318 --> 6.527434).  Saving model ...\n",
            "\titers: 10, epoch: 8 | loss: 2.6941631\n",
            "\tspeed: 0.4200s/iter; left time: 586.6900s\n",
            "\titers: 20, epoch: 8 | loss: 6.3048682\n",
            "\tspeed: 0.4208s/iter; left time: 583.5892s\n",
            "\titers: 30, epoch: 8 | loss: 3.6059780\n",
            "\tspeed: 0.4216s/iter; left time: 580.5185s\n",
            "\titers: 40, epoch: 8 | loss: 3.2882054\n",
            "\tspeed: 0.4207s/iter; left time: 575.1398s\n",
            "\titers: 50, epoch: 8 | loss: 3.4632587\n",
            "\tspeed: 0.4201s/iter; left time: 570.0693s\n",
            "\titers: 60, epoch: 8 | loss: 3.3321564\n",
            "\tspeed: 0.4212s/iter; left time: 567.3379s\n",
            "\titers: 70, epoch: 8 | loss: 2.9810927\n",
            "\tspeed: 0.4216s/iter; left time: 563.7216s\n",
            "\titers: 80, epoch: 8 | loss: 6.1003509\n",
            "\tspeed: 0.4192s/iter; left time: 556.3031s\n",
            "\titers: 90, epoch: 8 | loss: 3.8469124\n",
            "\tspeed: 0.4199s/iter; left time: 553.0715s\n",
            "\titers: 100, epoch: 8 | loss: 3.8223341\n",
            "\tspeed: 0.4212s/iter; left time: 550.4860s\n",
            "\titers: 110, epoch: 8 | loss: 4.7121925\n",
            "\tspeed: 0.4195s/iter; left time: 544.0962s\n",
            "\titers: 120, epoch: 8 | loss: 4.2870774\n",
            "\tspeed: 0.4214s/iter; left time: 542.3533s\n",
            "\titers: 130, epoch: 8 | loss: 2.1211202\n",
            "\tspeed: 0.4237s/iter; left time: 541.0586s\n",
            "\titers: 140, epoch: 8 | loss: 2.7174356\n",
            "\tspeed: 0.4203s/iter; left time: 532.5048s\n",
            "\titers: 150, epoch: 8 | loss: 5.8435445\n",
            "\tspeed: 0.4202s/iter; left time: 528.2121s\n",
            "\titers: 160, epoch: 8 | loss: 2.9552214\n",
            "\tspeed: 0.4206s/iter; left time: 524.5493s\n",
            "\titers: 170, epoch: 8 | loss: 3.4432948\n",
            "\tspeed: 0.4204s/iter; left time: 520.0491s\n",
            "\titers: 180, epoch: 8 | loss: 7.6914678\n",
            "\tspeed: 0.4219s/iter; left time: 517.6486s\n",
            "\titers: 190, epoch: 8 | loss: 4.9357252\n",
            "\tspeed: 0.4185s/iter; left time: 509.3292s\n",
            "\titers: 200, epoch: 8 | loss: 3.9412248\n",
            "\tspeed: 0.4193s/iter; left time: 506.1034s\n",
            "\titers: 210, epoch: 8 | loss: 2.6404674\n",
            "\tspeed: 0.4246s/iter; left time: 508.2928s\n",
            "\titers: 220, epoch: 8 | loss: 2.9597216\n",
            "\tspeed: 0.4299s/iter; left time: 510.2515s\n",
            "\titers: 230, epoch: 8 | loss: 3.5390797\n",
            "\tspeed: 0.4232s/iter; left time: 498.1359s\n",
            "\titers: 240, epoch: 8 | loss: 5.1679358\n",
            "\tspeed: 0.4244s/iter; left time: 495.2634s\n",
            "\titers: 250, epoch: 8 | loss: 3.5711467\n",
            "\tspeed: 0.4216s/iter; left time: 487.7812s\n",
            "\titers: 260, epoch: 8 | loss: 3.6456344\n",
            "\tspeed: 0.4212s/iter; left time: 483.0618s\n",
            "\titers: 270, epoch: 8 | loss: 4.5113482\n",
            "\tspeed: 0.4205s/iter; left time: 478.1405s\n",
            "\titers: 280, epoch: 8 | loss: 5.7196527\n",
            "\tspeed: 0.4244s/iter; left time: 478.2699s\n",
            "\titers: 290, epoch: 8 | loss: 3.5163281\n",
            "\tspeed: 0.4221s/iter; left time: 471.4475s\n",
            "\titers: 300, epoch: 8 | loss: 2.7861481\n",
            "\tspeed: 0.4224s/iter; left time: 467.6200s\n",
            "\titers: 310, epoch: 8 | loss: 1.9911088\n",
            "\tspeed: 0.4206s/iter; left time: 461.4203s\n",
            "\titers: 320, epoch: 8 | loss: 4.2319026\n",
            "\tspeed: 0.4213s/iter; left time: 457.9547s\n",
            "\titers: 330, epoch: 8 | loss: 4.7261243\n",
            "\tspeed: 0.4227s/iter; left time: 455.2315s\n",
            "\titers: 340, epoch: 8 | loss: 3.9217045\n",
            "\tspeed: 0.4225s/iter; left time: 450.8240s\n",
            "\titers: 350, epoch: 8 | loss: 2.3196261\n",
            "\tspeed: 0.4199s/iter; left time: 443.8259s\n",
            "\titers: 360, epoch: 8 | loss: 4.4631057\n",
            "\tspeed: 0.4209s/iter; left time: 440.6325s\n",
            "\titers: 370, epoch: 8 | loss: 4.0376787\n",
            "\tspeed: 0.4241s/iter; left time: 439.7780s\n",
            "\titers: 380, epoch: 8 | loss: 4.7527561\n",
            "\tspeed: 0.4214s/iter; left time: 432.7525s\n",
            "\titers: 390, epoch: 8 | loss: 3.1743095\n",
            "\tspeed: 0.4220s/iter; left time: 429.1670s\n",
            "\titers: 400, epoch: 8 | loss: 5.1338668\n",
            "\tspeed: 0.4215s/iter; left time: 424.4920s\n",
            "\titers: 410, epoch: 8 | loss: 5.5202522\n",
            "\tspeed: 0.4222s/iter; left time: 420.9802s\n",
            "\titers: 420, epoch: 8 | loss: 3.8540103\n",
            "\tspeed: 0.4222s/iter; left time: 416.7280s\n",
            "\titers: 430, epoch: 8 | loss: 3.5775549\n",
            "\tspeed: 0.4208s/iter; left time: 411.0975s\n",
            "\titers: 440, epoch: 8 | loss: 7.4077706\n",
            "\tspeed: 0.4222s/iter; left time: 408.2575s\n",
            "\titers: 450, epoch: 8 | loss: 2.9241819\n",
            "\tspeed: 0.4203s/iter; left time: 402.2041s\n",
            "\titers: 460, epoch: 8 | loss: 2.6600385\n",
            "\tspeed: 0.4268s/iter; left time: 404.1652s\n",
            "\titers: 470, epoch: 8 | loss: 6.7066360\n",
            "\tspeed: 0.4250s/iter; left time: 398.1787s\n",
            "\titers: 480, epoch: 8 | loss: 3.3914099\n",
            "\tspeed: 0.4220s/iter; left time: 391.2223s\n",
            "\titers: 490, epoch: 8 | loss: 4.3743196\n",
            "\tspeed: 0.4203s/iter; left time: 385.4426s\n",
            "\titers: 500, epoch: 8 | loss: 3.5561397\n",
            "\tspeed: 0.4207s/iter; left time: 381.5358s\n",
            "\titers: 510, epoch: 8 | loss: 4.3227172\n",
            "\tspeed: 0.4217s/iter; left time: 378.2330s\n",
            "\titers: 520, epoch: 8 | loss: 2.2446740\n",
            "\tspeed: 0.4229s/iter; left time: 375.1432s\n",
            "\titers: 530, epoch: 8 | loss: 2.9478910\n",
            "\tspeed: 0.4209s/iter; left time: 369.1185s\n",
            "\titers: 540, epoch: 8 | loss: 2.2227032\n",
            "\tspeed: 0.4220s/iter; left time: 365.8909s\n",
            "\titers: 550, epoch: 8 | loss: 4.6082668\n",
            "\tspeed: 0.4200s/iter; left time: 359.9748s\n",
            "\titers: 560, epoch: 8 | loss: 3.1530621\n",
            "\tspeed: 0.4198s/iter; left time: 355.5354s\n",
            "\titers: 570, epoch: 8 | loss: 3.7861240\n",
            "\tspeed: 0.4229s/iter; left time: 353.9298s\n",
            "\titers: 580, epoch: 8 | loss: 3.1189413\n",
            "\tspeed: 0.4248s/iter; left time: 351.3259s\n",
            "\titers: 590, epoch: 8 | loss: 6.7715130\n",
            "\tspeed: 0.4226s/iter; left time: 345.2552s\n",
            "\titers: 600, epoch: 8 | loss: 2.1720819\n",
            "\tspeed: 0.4214s/iter; left time: 340.0660s\n",
            "\titers: 610, epoch: 8 | loss: 2.3845952\n",
            "\tspeed: 0.4218s/iter; left time: 336.1360s\n",
            "\titers: 620, epoch: 8 | loss: 4.7371726\n",
            "\tspeed: 0.4215s/iter; left time: 331.7545s\n",
            "\titers: 630, epoch: 8 | loss: 4.5602183\n",
            "\tspeed: 0.4207s/iter; left time: 326.8491s\n",
            "\titers: 640, epoch: 8 | loss: 7.5740743\n",
            "\tspeed: 0.4221s/iter; left time: 323.7201s\n",
            "\titers: 650, epoch: 8 | loss: 2.9319241\n",
            "\tspeed: 0.4234s/iter; left time: 320.5413s\n",
            "\titers: 660, epoch: 8 | loss: 2.7989900\n",
            "\tspeed: 0.4224s/iter; left time: 315.5023s\n",
            "\titers: 670, epoch: 8 | loss: 3.4267781\n",
            "\tspeed: 0.4186s/iter; left time: 308.4913s\n",
            "\titers: 680, epoch: 8 | loss: 3.7491047\n",
            "\tspeed: 0.4223s/iter; left time: 307.0048s\n",
            "\titers: 690, epoch: 8 | loss: 3.0806501\n",
            "\tspeed: 0.4246s/iter; left time: 304.4658s\n",
            "\titers: 700, epoch: 8 | loss: 3.7635453\n",
            "\tspeed: 0.4289s/iter; left time: 303.2323s\n",
            "Epoch: 8, Train Loss: 3.8492, Validation Loss: 6.2077\n",
            "Validation loss decreased (6.527434 --> 6.207655).  Saving model ...\n",
            "\titers: 10, epoch: 9 | loss: 4.7320294\n",
            "\tspeed: 0.4203s/iter; left time: 291.6682s\n",
            "\titers: 20, epoch: 9 | loss: 3.5957625\n",
            "\tspeed: 0.4220s/iter; left time: 288.6549s\n",
            "\titers: 30, epoch: 9 | loss: 3.4206550\n",
            "\tspeed: 0.4232s/iter; left time: 285.2553s\n",
            "\titers: 40, epoch: 9 | loss: 6.6798253\n",
            "\tspeed: 0.4230s/iter; left time: 280.8548s\n",
            "\titers: 50, epoch: 9 | loss: 4.7380757\n",
            "\tspeed: 0.4224s/iter; left time: 276.2664s\n",
            "\titers: 60, epoch: 9 | loss: 4.0880132\n",
            "\tspeed: 0.4225s/iter; left time: 272.0959s\n",
            "\titers: 70, epoch: 9 | loss: 3.3191254\n",
            "\tspeed: 0.4230s/iter; left time: 268.2136s\n",
            "\titers: 80, epoch: 9 | loss: 3.0346258\n",
            "\tspeed: 0.4206s/iter; left time: 262.4719s\n",
            "\titers: 90, epoch: 9 | loss: 4.6569123\n",
            "\tspeed: 0.4225s/iter; left time: 259.4179s\n",
            "\titers: 100, epoch: 9 | loss: 3.1383460\n",
            "\tspeed: 0.4285s/iter; left time: 258.7987s\n",
            "\titers: 110, epoch: 9 | loss: 3.9363792\n",
            "\tspeed: 0.4231s/iter; left time: 251.3165s\n",
            "\titers: 120, epoch: 9 | loss: 3.0589936\n",
            "\tspeed: 0.4218s/iter; left time: 246.3350s\n",
            "\titers: 130, epoch: 9 | loss: 3.0698280\n",
            "\tspeed: 0.4214s/iter; left time: 241.8708s\n",
            "\titers: 140, epoch: 9 | loss: 3.4570215\n",
            "\tspeed: 0.4219s/iter; left time: 237.9523s\n",
            "\titers: 150, epoch: 9 | loss: 2.2343347\n",
            "\tspeed: 0.4197s/iter; left time: 232.5079s\n",
            "\titers: 160, epoch: 9 | loss: 4.1777263\n",
            "\tspeed: 0.4201s/iter; left time: 228.5497s\n",
            "\titers: 170, epoch: 9 | loss: 4.5949998\n",
            "\tspeed: 0.4210s/iter; left time: 224.8182s\n",
            "\titers: 180, epoch: 9 | loss: 6.5132866\n",
            "\tspeed: 0.4203s/iter; left time: 220.2382s\n",
            "\titers: 190, epoch: 9 | loss: 3.8112109\n",
            "\tspeed: 0.4225s/iter; left time: 217.1471s\n",
            "\titers: 200, epoch: 9 | loss: 3.4477055\n",
            "\tspeed: 0.4219s/iter; left time: 212.6519s\n",
            "\titers: 210, epoch: 9 | loss: 3.2499163\n",
            "\tspeed: 0.4279s/iter; left time: 211.3848s\n",
            "\titers: 220, epoch: 9 | loss: 2.5029452\n",
            "\tspeed: 0.4254s/iter; left time: 205.8915s\n",
            "\titers: 230, epoch: 9 | loss: 2.8858998\n",
            "\tspeed: 0.4224s/iter; left time: 200.2077s\n",
            "\titers: 240, epoch: 9 | loss: 2.6216760\n",
            "\tspeed: 0.4218s/iter; left time: 195.7236s\n",
            "\titers: 250, epoch: 9 | loss: 3.8221309\n",
            "\tspeed: 0.4228s/iter; left time: 191.9412s\n",
            "\titers: 260, epoch: 9 | loss: 3.3528764\n",
            "\tspeed: 0.4225s/iter; left time: 187.5879s\n",
            "\titers: 270, epoch: 9 | loss: 5.6892605\n",
            "\tspeed: 0.4239s/iter; left time: 183.9687s\n",
            "\titers: 280, epoch: 9 | loss: 4.7482352\n",
            "\tspeed: 0.4198s/iter; left time: 177.9775s\n",
            "\titers: 290, epoch: 9 | loss: 4.3180289\n",
            "\tspeed: 0.4233s/iter; left time: 175.2536s\n",
            "\titers: 300, epoch: 9 | loss: 3.6924925\n",
            "\tspeed: 0.4278s/iter; left time: 172.8367s\n",
            "\titers: 310, epoch: 9 | loss: 3.4088981\n",
            "\tspeed: 0.4223s/iter; left time: 166.3868s\n",
            "\titers: 320, epoch: 9 | loss: 2.4013197\n",
            "\tspeed: 0.4326s/iter; left time: 166.1323s\n",
            "\titers: 330, epoch: 9 | loss: 2.6699886\n",
            "\tspeed: 0.4263s/iter; left time: 159.4378s\n",
            "\titers: 340, epoch: 9 | loss: 2.4409144\n",
            "\tspeed: 0.4258s/iter; left time: 154.9826s\n",
            "\titers: 350, epoch: 9 | loss: 4.0911527\n",
            "\tspeed: 0.4303s/iter; left time: 152.3224s\n",
            "\titers: 360, epoch: 9 | loss: 4.3840714\n",
            "\tspeed: 0.4388s/iter; left time: 150.9341s\n",
            "\titers: 370, epoch: 9 | loss: 3.6272981\n",
            "\tspeed: 0.4524s/iter; left time: 151.0926s\n",
            "\titers: 380, epoch: 9 | loss: 4.4231796\n",
            "\tspeed: 0.4435s/iter; left time: 143.6930s\n",
            "\titers: 390, epoch: 9 | loss: 3.5837200\n",
            "\tspeed: 0.4264s/iter; left time: 133.9042s\n",
            "\titers: 400, epoch: 9 | loss: 2.2373497\n",
            "\tspeed: 0.4272s/iter; left time: 129.8773s\n",
            "\titers: 410, epoch: 9 | loss: 3.8972969\n",
            "\tspeed: 0.4299s/iter; left time: 126.3892s\n",
            "\titers: 420, epoch: 9 | loss: 1.9798416\n",
            "\tspeed: 0.4279s/iter; left time: 121.5247s\n",
            "\titers: 430, epoch: 9 | loss: 4.4214950\n",
            "\tspeed: 0.4315s/iter; left time: 118.2176s\n",
            "\titers: 440, epoch: 9 | loss: 3.6419821\n",
            "\tspeed: 0.4294s/iter; left time: 113.3721s\n",
            "\titers: 450, epoch: 9 | loss: 3.9435551\n",
            "\tspeed: 0.4295s/iter; left time: 109.0892s\n",
            "\titers: 460, epoch: 9 | loss: 5.8393917\n",
            "\tspeed: 0.4310s/iter; left time: 105.1632s\n",
            "\titers: 470, epoch: 9 | loss: 2.4527178\n",
            "\tspeed: 0.4287s/iter; left time: 100.3271s\n",
            "\titers: 480, epoch: 9 | loss: 5.6509075\n",
            "\tspeed: 0.4284s/iter; left time: 95.9587s\n",
            "\titers: 490, epoch: 9 | loss: 2.6911480\n",
            "\tspeed: 0.4287s/iter; left time: 91.7512s\n",
            "\titers: 500, epoch: 9 | loss: 3.6999910\n",
            "\tspeed: 0.4297s/iter; left time: 87.6584s\n",
            "\titers: 510, epoch: 9 | loss: 2.7304716\n",
            "\tspeed: 0.4275s/iter; left time: 82.9432s\n",
            "\titers: 520, epoch: 9 | loss: 3.3479779\n",
            "\tspeed: 0.4294s/iter; left time: 79.0156s\n",
            "\titers: 530, epoch: 9 | loss: 3.5708427\n",
            "\tspeed: 0.4296s/iter; left time: 74.7589s\n",
            "\titers: 540, epoch: 9 | loss: 4.2335849\n",
            "\tspeed: 0.4275s/iter; left time: 70.1021s\n",
            "\titers: 550, epoch: 9 | loss: 3.6826341\n",
            "\tspeed: 0.4276s/iter; left time: 65.8549s\n",
            "\titers: 560, epoch: 9 | loss: 4.1531768\n",
            "\tspeed: 0.4287s/iter; left time: 61.7311s\n",
            "\titers: 570, epoch: 9 | loss: 4.5448341\n",
            "\tspeed: 0.4309s/iter; left time: 57.7419s\n",
            "\titers: 580, epoch: 9 | loss: 3.7601497\n",
            "\tspeed: 0.4281s/iter; left time: 53.0824s\n",
            "\titers: 590, epoch: 9 | loss: 4.4152722\n",
            "\tspeed: 0.4287s/iter; left time: 48.8754s\n",
            "\titers: 600, epoch: 9 | loss: 5.5915322\n",
            "\tspeed: 0.4375s/iter; left time: 45.5008s\n",
            "\titers: 610, epoch: 9 | loss: 2.8559906\n",
            "\tspeed: 0.4324s/iter; left time: 40.6458s\n",
            "\titers: 620, epoch: 9 | loss: 3.6103737\n",
            "\tspeed: 0.4287s/iter; left time: 36.0117s\n",
            "\titers: 630, epoch: 9 | loss: 5.4570289\n",
            "\tspeed: 0.4274s/iter; left time: 31.6268s\n",
            "\titers: 640, epoch: 9 | loss: 2.6481631\n",
            "\tspeed: 0.4256s/iter; left time: 27.2414s\n",
            "\titers: 650, epoch: 9 | loss: 2.9089639\n",
            "\tspeed: 0.4247s/iter; left time: 22.9349s\n",
            "\titers: 660, epoch: 9 | loss: 3.9622719\n",
            "\tspeed: 0.4266s/iter; left time: 18.7702s\n",
            "\titers: 670, epoch: 9 | loss: 4.2938724\n",
            "\tspeed: 0.4270s/iter; left time: 14.5171s\n",
            "\titers: 680, epoch: 9 | loss: 2.3174901\n",
            "\tspeed: 0.4271s/iter; left time: 10.2505s\n",
            "\titers: 690, epoch: 9 | loss: 3.2548180\n",
            "\tspeed: 0.4260s/iter; left time: 5.9643s\n",
            "\titers: 700, epoch: 9 | loss: 4.4293633\n",
            "\tspeed: 0.4265s/iter; left time: 1.7062s\n",
            "Epoch: 9, Train Loss: 3.6974, Validation Loss: 6.1115\n",
            "Validation loss decreased (6.207655 --> 6.111547).  Saving model ...\n",
            "\titers: 10, epoch: 10 | loss: 4.1058841\n",
            "\tspeed: 0.4264s/iter; left time: -3.8379s\n",
            "\titers: 20, epoch: 10 | loss: 3.9116478\n",
            "\tspeed: 0.4262s/iter; left time: -8.0979s\n",
            "\titers: 30, epoch: 10 | loss: 2.5487487\n",
            "\tspeed: 0.4274s/iter; left time: -12.3952s\n",
            "\titers: 40, epoch: 10 | loss: 4.6070619\n",
            "\tspeed: 0.4262s/iter; left time: -16.6207s\n",
            "\titers: 50, epoch: 10 | loss: 2.9686239\n",
            "\tspeed: 0.4272s/iter; left time: -20.9351s\n",
            "\titers: 60, epoch: 10 | loss: 4.1008039\n",
            "\tspeed: 0.4266s/iter; left time: -25.1692s\n",
            "\titers: 70, epoch: 10 | loss: 3.8469725\n",
            "\tspeed: 0.4261s/iter; left time: -29.3984s\n",
            "\titers: 80, epoch: 10 | loss: 2.9511602\n",
            "\tspeed: 0.4280s/iter; left time: -33.8104s\n",
            "\titers: 90, epoch: 10 | loss: 2.8108845\n",
            "\tspeed: 0.4256s/iter; left time: -37.8795s\n",
            "\titers: 100, epoch: 10 | loss: 4.8471951\n",
            "\tspeed: 0.4240s/iter; left time: -41.9715s\n",
            "\titers: 110, epoch: 10 | loss: 4.4764400\n",
            "\tspeed: 0.4239s/iter; left time: -46.2069s\n",
            "\titers: 120, epoch: 10 | loss: 3.6121426\n",
            "\tspeed: 0.4268s/iter; left time: -50.7908s\n",
            "\titers: 130, epoch: 10 | loss: 4.1176414\n",
            "\tspeed: 0.4229s/iter; left time: -54.5562s\n",
            "\titers: 140, epoch: 10 | loss: 2.0997224\n",
            "\tspeed: 0.4227s/iter; left time: -58.7552s\n",
            "\titers: 150, epoch: 10 | loss: 6.3220801\n",
            "\tspeed: 0.4217s/iter; left time: -62.8355s\n",
            "\titers: 160, epoch: 10 | loss: 2.0881622\n",
            "\tspeed: 0.4208s/iter; left time: -66.9131s\n",
            "\titers: 170, epoch: 10 | loss: 4.5736289\n",
            "\tspeed: 0.4256s/iter; left time: -71.9322s\n",
            "\titers: 180, epoch: 10 | loss: 2.5695956\n",
            "\tspeed: 0.4218s/iter; left time: -75.5036s\n",
            "\titers: 190, epoch: 10 | loss: 2.3874273\n",
            "\tspeed: 0.4214s/iter; left time: -79.6411s\n",
            "\titers: 200, epoch: 10 | loss: 3.2983949\n",
            "\tspeed: 0.4224s/iter; left time: -84.0522s\n",
            "\titers: 210, epoch: 10 | loss: 5.3313904\n",
            "\tspeed: 0.4202s/iter; left time: -87.8193s\n",
            "\titers: 220, epoch: 10 | loss: 3.0966291\n",
            "\tspeed: 0.4213s/iter; left time: -92.2722s\n",
            "\titers: 230, epoch: 10 | loss: 3.9442215\n",
            "\tspeed: 0.4221s/iter; left time: -96.6587s\n",
            "\titers: 240, epoch: 10 | loss: 4.8194222\n",
            "\tspeed: 0.4214s/iter; left time: -100.7159s\n",
            "\titers: 250, epoch: 10 | loss: 3.2527821\n",
            "\tspeed: 0.4224s/iter; left time: -105.1815s\n",
            "\titers: 260, epoch: 10 | loss: 4.4256654\n",
            "\tspeed: 0.4215s/iter; left time: -109.1621s\n",
            "\titers: 270, epoch: 10 | loss: 2.6858375\n",
            "\tspeed: 0.4234s/iter; left time: -113.8877s\n",
            "\titers: 280, epoch: 10 | loss: 2.1872253\n",
            "\tspeed: 0.4230s/iter; left time: -118.0277s\n",
            "\titers: 290, epoch: 10 | loss: 3.0221131\n",
            "\tspeed: 0.4217s/iter; left time: -121.8593s\n",
            "\titers: 300, epoch: 10 | loss: 4.2539639\n",
            "\tspeed: 0.4215s/iter; left time: -126.0411s\n",
            "\titers: 310, epoch: 10 | loss: 2.1936045\n",
            "\tspeed: 0.4219s/iter; left time: -130.3670s\n",
            "\titers: 320, epoch: 10 | loss: 3.1071966\n",
            "\tspeed: 0.4235s/iter; left time: -135.1075s\n",
            "\titers: 330, epoch: 10 | loss: 3.2592535\n",
            "\tspeed: 0.4215s/iter; left time: -138.6657s\n",
            "\titers: 340, epoch: 10 | loss: 2.5360761\n",
            "\tspeed: 0.4214s/iter; left time: -142.8459s\n",
            "\titers: 350, epoch: 10 | loss: 3.5375450\n",
            "\tspeed: 0.4205s/iter; left time: -146.7508s\n",
            "\titers: 360, epoch: 10 | loss: 2.9421425\n",
            "\tspeed: 0.4213s/iter; left time: -151.2541s\n",
            "\titers: 370, epoch: 10 | loss: 4.1342425\n",
            "\tspeed: 0.4207s/iter; left time: -155.2338s\n",
            "\titers: 380, epoch: 10 | loss: 3.9346645\n",
            "\tspeed: 0.4200s/iter; left time: -159.1804s\n",
            "\titers: 390, epoch: 10 | loss: 2.8950469\n",
            "\tspeed: 0.4194s/iter; left time: -163.1537s\n",
            "\titers: 400, epoch: 10 | loss: 3.9916146\n",
            "\tspeed: 0.4195s/iter; left time: -167.3902s\n",
            "\titers: 410, epoch: 10 | loss: 3.5979929\n",
            "\tspeed: 0.4223s/iter; left time: -172.7109s\n",
            "\titers: 420, epoch: 10 | loss: 2.6483815\n",
            "\tspeed: 0.4199s/iter; left time: -175.9244s\n",
            "\titers: 430, epoch: 10 | loss: 3.5236864\n",
            "\tspeed: 0.4209s/iter; left time: -180.5563s\n",
            "\titers: 440, epoch: 10 | loss: 2.8431320\n",
            "\tspeed: 0.4212s/iter; left time: -184.9184s\n",
            "\titers: 450, epoch: 10 | loss: 4.6018500\n",
            "\tspeed: 0.4205s/iter; left time: -188.7973s\n",
            "\titers: 460, epoch: 10 | loss: 3.7990105\n",
            "\tspeed: 0.4213s/iter; left time: -193.3863s\n",
            "\titers: 470, epoch: 10 | loss: 4.4322157\n",
            "\tspeed: 0.4252s/iter; left time: -199.4142s\n",
            "\titers: 480, epoch: 10 | loss: 6.0381279\n",
            "\tspeed: 0.4193s/iter; left time: -200.8619s\n",
            "\titers: 490, epoch: 10 | loss: 2.4579561\n",
            "\tspeed: 0.4212s/iter; left time: -205.9463s\n",
            "\titers: 500, epoch: 10 | loss: 4.2835641\n",
            "\tspeed: 0.4215s/iter; left time: -210.3214s\n",
            "\titers: 510, epoch: 10 | loss: 3.3335207\n",
            "\tspeed: 0.4229s/iter; left time: -215.2564s\n",
            "\titers: 520, epoch: 10 | loss: 5.2878232\n",
            "\tspeed: 0.4199s/iter; left time: -217.9087s\n",
            "\titers: 530, epoch: 10 | loss: 3.1797965\n",
            "\tspeed: 0.4202s/iter; left time: -222.2621s\n",
            "\titers: 540, epoch: 10 | loss: 4.3491282\n",
            "\tspeed: 0.4220s/iter; left time: -227.4847s\n",
            "\titers: 550, epoch: 10 | loss: 3.4634092\n",
            "\tspeed: 0.4223s/iter; left time: -231.8200s\n",
            "\titers: 560, epoch: 10 | loss: 4.2218199\n",
            "\tspeed: 0.4210s/iter; left time: -235.3336s\n",
            "\titers: 570, epoch: 10 | loss: 3.5335720\n",
            "\tspeed: 0.4212s/iter; left time: -239.6665s\n",
            "\titers: 580, epoch: 10 | loss: 3.9657717\n",
            "\tspeed: 0.4216s/iter; left time: -244.0894s\n",
            "\titers: 590, epoch: 10 | loss: 5.9018631\n",
            "\tspeed: 0.4211s/iter; left time: -248.0113s\n",
            "\titers: 600, epoch: 10 | loss: 4.5743709\n",
            "\tspeed: 0.4200s/iter; left time: -251.5958s\n",
            "\titers: 610, epoch: 10 | loss: 3.2251565\n",
            "\tspeed: 0.4211s/iter; left time: -256.4654s\n",
            "\titers: 620, epoch: 10 | loss: 2.8087065\n",
            "\tspeed: 0.4214s/iter; left time: -260.8765s\n",
            "\titers: 630, epoch: 10 | loss: 3.2462471\n",
            "\tspeed: 0.4215s/iter; left time: -265.1327s\n",
            "\titers: 640, epoch: 10 | loss: 3.7456522\n",
            "\tspeed: 0.4213s/iter; left time: -269.2026s\n",
            "\titers: 650, epoch: 10 | loss: 3.9949348\n",
            "\tspeed: 0.4277s/iter; left time: -277.5864s\n",
            "\titers: 660, epoch: 10 | loss: 5.3128204\n",
            "\tspeed: 0.4254s/iter; left time: -280.3374s\n",
            "\titers: 670, epoch: 10 | loss: 3.4034674\n",
            "\tspeed: 0.4205s/iter; left time: -281.3187s\n",
            "\titers: 680, epoch: 10 | loss: 5.0049510\n",
            "\tspeed: 0.4208s/iter; left time: -285.7033s\n",
            "\titers: 690, epoch: 10 | loss: 2.3405664\n",
            "\tspeed: 0.4216s/iter; left time: -290.4932s\n",
            "\titers: 700, epoch: 10 | loss: 3.4416180\n",
            "\tspeed: 0.4232s/iter; left time: -295.8400s\n",
            "Epoch: 10, Train Loss: 3.6108, Validation Loss: 5.9244\n",
            "Validation loss decreased (6.111547 --> 5.924426).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def build_model():\n",
        "    model_dict = {\n",
        "        'micn': MICN,\n",
        "    }\n",
        "    model = model_dict[MODEL](\n",
        "        DEC_IN,\n",
        "        C_OUT,\n",
        "        SEQ_LEN,\n",
        "        LABEL_LEN,\n",
        "        PRED_LEN,\n",
        "        D_MODEL,\n",
        "        N_HEADS,\n",
        "        D_LAYERS,\n",
        "        DROPOUT,\n",
        "        EMBED,\n",
        "        FREQ,\n",
        "        DEVICE,\n",
        "        MODE,\n",
        "        DECOMP_KERNEL,\n",
        "        CONV_KERNEL,\n",
        "        ISOMETRIC_KERNEL\n",
        "    ).float()\n",
        "    return model\n",
        "\n",
        "# Optimizer and loss function\n",
        "def select_optimizer(model):\n",
        "    return torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def select_criterion():\n",
        "    return nn.MSELoss()\n",
        "\n",
        "# Validation function\n",
        "def validate(model, vali_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:\n",
        "            pred, true = process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "            loss = criterion(pred, true)\n",
        "            total_loss.append(loss.item())\n",
        "    return np.average(total_loss)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, vali_loader, epochs=100):\n",
        "    optimizer = select_optimizer(model)\n",
        "    criterion = select_criterion()\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    print('===== Started Model training =====')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        iter_count = 0\n",
        "        time_now = time.time()\n",
        "\n",
        "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            pred, true = process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "            loss = criterion(pred, true)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "            iter_count += 1\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"\\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f}\")\n",
        "                speed = (time.time() - time_now) / iter_count\n",
        "                left_time = speed * ((epochs - epoch - 1) * len(train_loader) - i)\n",
        "                print(f'\\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')\n",
        "                iter_count = 0\n",
        "                time_now = time.time()\n",
        "\n",
        "        avg_train_loss = np.average(train_loss)\n",
        "        vali_loss = validate(model, vali_loader, criterion)\n",
        "        print(f'Epoch: {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {vali_loss:.4f}')\n",
        "\n",
        "        early_stopping(vali_loss, model, \"checkpoints\")\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "def process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
        "    batch_x = batch_x.float()\n",
        "    batch_y = batch_y.float()\n",
        "    batch_x_mark = batch_x_mark.float()\n",
        "    batch_y_mark = batch_y_mark.float()\n",
        "\n",
        "    # Prepare decoder input\n",
        "    # Ensure dec_inp has the correct size for the model\n",
        "    dec_inp = torch.zeros([batch_y.shape[0], PRED_LEN, batch_y.shape[-1]]).float()  # Adjust padding if needed\n",
        "    dec_inp = torch.cat([batch_y[:, :LABEL_LEN, :], dec_inp], dim=1)\n",
        "\n",
        "\n",
        "    # Get model outputs\n",
        "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "    return outputs, batch_y[:, -PRED_LEN:, :]  # Returnin\n",
        "\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = build_model()\n",
        "\n",
        "# Train model\n",
        "train(model, train_loader, vali_loader, epochs=10)  # Adjust epochs as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LE3VVSXc-8VK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics: MSE: 5.9295068, RMSE: 2.4350579, MAE: 1.5298474, MAPE: 0.8561473, R²: 0.9352938, MASE: 5.7369561, SMAPE: 0.8601221\n",
            "Test Metrics: MSE: 11.7598267, RMSE: 3.4292603, MAE: 2.4420907, MAPE: 1.1104506, R²: 0.8584346, MASE: 8.4439907, SMAPE: 1.1163028\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def vali(model, d_loader, criterion):\n",
        "    preds, true_vals = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, batch_x_mark, batch_y_mark in d_loader:\n",
        "            batch_x = batch_x.float().to(DEVICE)\n",
        "            batch_y = batch_y[:, -PRED_LEN:, :].float().to(DEVICE)\n",
        "            batch_x_mark = batch_x_mark.float().to(DEVICE)\n",
        "            batch_y_mark = batch_y_mark.float().to(DEVICE)\n",
        "\n",
        "\n",
        "            dec_inp = torch.zeros([batch_y.shape[0], PRED_LEN, batch_y.shape[-1]], device=DEVICE).float()\n",
        "            dec_inp = torch.cat([batch_y[:, :LABEL_LEN, :], dec_inp], dim=1)\n",
        "\n",
        "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "            outputs = outputs[:, -PRED_LEN:, :]\n",
        "\n",
        "            preds.append(outputs.detach().cpu().numpy())\n",
        "            true_vals.append(batch_y.detach().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "    if preds.ndim > 2:\n",
        "        preds = preds.reshape(-1, preds.shape[-1])\n",
        "    if true_vals.ndim > 2:\n",
        "        true_vals = true_vals.reshape(-1, true_vals.shape[-1])\n",
        "\n",
        "    mse = mean_squared_error(true_vals, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(true_vals, preds)\n",
        "    mape = np.mean(np.abs((true_vals - preds) / true_vals)) * 100\n",
        "    r2 = r2_score(true_vals, preds)\n",
        "\n",
        "    naive_forecast = np.mean(np.abs(true_vals[:-1] - true_vals[1:]))\n",
        "    mase = mae / naive_forecast if naive_forecast != 0 else np.nan\n",
        "\n",
        "\n",
        "    smape = 100 * np.mean(2 * np.abs(preds - true_vals) / (np.abs(true_vals) + np.abs(preds) + 1e-10))\n",
        "\n",
        "    return {\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'mape': mape,\n",
        "        'r2': r2,\n",
        "        'mase': mase,\n",
        "        'smape': smape\n",
        "    }\n",
        "\n",
        "vali_metrics = vali(model, vali_loader, select_criterion())\n",
        "test_metrics = vali(model, test_loader, select_criterion())\n",
        "\n",
        "\n",
        "print(f\"Validation Metrics: MSE: {vali_metrics['mse']:.7f}, RMSE: {vali_metrics['rmse']:.7f}, MAE: {vali_metrics['mae']:.7f}, \"\n",
        "      f\"MAPE: {vali_metrics['mape']:.7f}, R²: {vali_metrics['r2']:.7f}, MASE: {vali_metrics['mase']:.7f}, SMAPE: {vali_metrics['smape']:.7f}\")\n",
        "\n",
        "print(f\"Test Metrics: MSE: {test_metrics['mse']:.7f}, RMSE: {test_metrics['rmse']:.7f}, MAE: {test_metrics['mae']:.7f}, \"\n",
        "      f\"MAPE: {test_metrics['mape']:.7f}, R²: {test_metrics['r2']:.7f}, MASE: {test_metrics['mase']:.7f}, SMAPE: {test_metrics['smape']:.7f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "X_batch, y_batch, X_mark, y_mark = next(iter(test_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    dec_inp = torch.zeros([y_batch.shape[0], PRED_LEN, y_batch.shape[-1]], device=DEVICE).float()\n",
        "    dec_inp = torch.cat([y_batch[:, :LABEL_LEN, :].to(DEVICE), dec_inp], dim=1)\n",
        "\n",
        "    lin_preds = model(X_batch.float().to(DEVICE), X_mark.float().to(DEVICE), dec_inp, y_mark.float().to(DEVICE)).detach().cpu().numpy()\n",
        "    lt_preds = lin_preds  \n",
        "\n",
        "gt = np.concatenate((X_batch[10, -96:, 0].cpu().numpy(), y_batch[10, -PRED_LEN:, 0].cpu().numpy()), axis=0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(list(range(-96, PRED_LEN)), gt, label='Ground Truth', linewidth=1.5)\n",
        "plt.plot(list(range(PRED_LEN)), lin_preds[10, :, 0], label='micn', color='red', linewidth=1)\n",
        "plt.axvline(x=0, color=\"k\", linestyle='--')  \n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title('Trading Data Sample Prediction')\n",
        "plt.legend()\n",
        "plt.grid() \n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "micn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
