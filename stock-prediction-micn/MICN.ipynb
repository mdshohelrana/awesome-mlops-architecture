{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = 'micn'\n",
    "MODE = 'regre'\n",
    "DATA = 'Trading'\n",
    "FEATURES = 'S'\n",
    "FREQ = 'h'\n",
    "CONV_KERNEL = [12, 16]\n",
    "D_LAYERS = 1\n",
    "D_MODEL = 512\n",
    "SEQ_LEN = 96\n",
    "DATA_PATH = '5m_intraday_data.csv'\n",
    "TARGET = 'Close'\n",
    "ENC_IN = 1\n",
    "DEC_IN = 1\n",
    "C_OUT = 1\n",
    "LABEL_LEN = 96\n",
    "PRED_LEN = 96\n",
    "BATCH_SIZE= 32\n",
    "N_HEADS = 8\n",
    "DROPOUT = 0.05\n",
    "EMBED = 'timeF'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DECOMP_KERNEL = [17, 23]\n",
    "ISOMETRIC_KERNEL = [17, 23]\n",
    "INVERSE = False\n",
    "OUTPUT_ATTENTION = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj=='type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.75 ** ((epoch-1) // 1))}\n",
    "    elif args.lradj=='type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-4, 4: 1e-4, 6: 5e-5, 8: 1e-5,\n",
    "            10: 5e-6, 15: 1e-6, 20: 5e-7\n",
    "        }\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path+'/'+'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self):\n",
    "        self.mean = 0.\n",
    "        self.std = 1.\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.mean = data.mean(0)\n",
    "        self.std = data.std(0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
    "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
    "        return (data - mean) / std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
    "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
    "        return (data * std) + mean\n",
    "    \n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "def time_features(dates, timeenc=1, freq='h'):\n",
    "    \"\"\"\n",
    "    > `time_features` takes in a `dates` dataframe with a 'dates' column and extracts the date down to `freq` where freq can be any of the following if `timeenc` is 0: \n",
    "    > * m - [month]\n",
    "    > * w - [month]\n",
    "    > * d - [month, day, weekday]\n",
    "    > * b - [month, day, weekday]\n",
    "    > * h - [month, day, weekday, hour]\n",
    "    > * t - [month, day, weekday, hour, *minute]\n",
    "    > \n",
    "    > If `timeenc` is 1, a similar, but different list of `freq` values are supported (all encoded between [-0.5 and 0.5]): \n",
    "    > * Q - [month]\n",
    "    > * M - [month]\n",
    "    > * W - [Day of month, week of year]\n",
    "    > * D - [Day of week, day of month, day of year]\n",
    "    > * B - [Day of week, day of month, day of year]\n",
    "    > * H - [Hour of day, day of week, day of month, day of year]\n",
    "    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n",
    "    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n",
    "\n",
    "    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n",
    "    \"\"\"\n",
    "    if timeenc==0:\n",
    "        dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
    "        dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
    "        dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
    "        dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
    "        dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
    "        dates['minute'] = dates.minute.map(lambda x:x//15)\n",
    "        print(\"dates\", dates)\n",
    "        freq_map = {\n",
    "            'y':[],'m':['month'],'w':['month'],'d':['month','day','weekday'],\n",
    "            'b':['month','day','weekday'],'h':['month','day','weekday','hour'],\n",
    "            't':['month','day','weekday','hour','minute'],\n",
    "        }\n",
    "        print(dates[freq_map[freq.lower()]].values)\n",
    "        return dates[freq_map[freq.lower()]].values\n",
    "    if timeenc==1:\n",
    "        dates = pd.to_datetime(dates.date.values)\n",
    "        return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)]).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trading_dataset(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None, \n",
    "                 features='S', data_path='5m_intraday_data.csv', \n",
    "                 target='Close', scale=True, inverse=True, timeenc=0, freq='5T', cols=None):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        if size is None:\n",
    "            self.seq_len = SEQ_LEN  # Adjust based on your needs\n",
    "            self.label_len = LABEL_LEN\n",
    "            self.pred_len = PRED_LEN\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        \n",
    "        # Initialize dataset type\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.inverse = inverse\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.cols = cols\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # Load the CSV file\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "\n",
    "        # Dropping Gmtoffset column\n",
    "        df_raw = df_raw.drop(columns=['Gmtoffset'])\n",
    "\n",
    "        # Strip any spaces in column names just in case\n",
    "        df_raw.columns = df_raw.columns.str.strip()\n",
    "\n",
    "        # Check if the target column exists\n",
    "        if self.target not in df_raw.columns:\n",
    "            raise ValueError(f\"Target column '{self.target}' not found in the dataset\")\n",
    "\n",
    "        # Check for NaN values and replace them (customize this based on how you want to handle NaNs)\n",
    "        if df_raw.isnull().values.any():\n",
    "            print(\"NaN values found in the dataset, replacing with forward fill (ffill)\")\n",
    "            df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
    "        \n",
    "        # Select columns (features)\n",
    "        if self.cols:\n",
    "            cols = self.cols.copy()\n",
    "            if self.target in cols:\n",
    "                cols.remove(self.target)\n",
    "        else:\n",
    "            # Use all columns except Timestamp, Datetime, and target\n",
    "            cols = list(df_raw.columns)\n",
    "            cols.remove(self.target)  # Ensure the target column is in the list\n",
    "            cols.remove('Timestamp')  # Unix timestamp\n",
    "            cols.remove('Datetime')   # Human-readable date\n",
    "\n",
    "        # Reorder the dataframe to have Datetime, features, and target\n",
    "        df_raw = df_raw[['Datetime'] + cols + [self.target]]\n",
    "\n",
    "        # Train/Val/Test splits\n",
    "        num_train = int(len(df_raw) * 0.7)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        # Features and target data selection\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            # Multivariate: use all columns except target and Datetime\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            # Single feature: only use the target column\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        # Scaling the data if required\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]  # Only scale using the training data\n",
    "            train_data = np.nan_to_num(train_data)  # Replace NaNs and infinities with 0\n",
    "            self.scaler.fit(train_data)\n",
    "            data = self.scaler.transform(np.nan_to_num(df_data.values))  # Apply scaling to the entire data\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        # Processing the datetime column for timestamp encoding\n",
    "        df_stamp = df_raw[['Datetime']][border1:border2]\n",
    "        df_stamp = df_stamp.rename(columns={'Datetime': 'date'})  # Rename 'Datetime' to 'date' for compatibility\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp['date'])\n",
    "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq)\n",
    "\n",
    "        # Save the data for X (features) and y (target)\n",
    "        self.data_x = data[border1:border2]\n",
    "        if self.inverse:\n",
    "            self.data_y = df_data.values[border1:border2]  # Original scale for y if inverse\n",
    "        else:\n",
    "            self.data_y = data[border1:border2]  # Scaled version\n",
    "\n",
    "        # Store time-related features\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        # Ensure valid sequence boundaries to avoid out-of-bounds indexing\n",
    "        if s_end > len(self.data_x) or r_end > len(self.data_x):\n",
    "            raise IndexError(f\"Index out of bounds: s_end={s_end}, r_end={r_end}\")\n",
    "\n",
    "        # Getting sequences for X (input) and Y (target)\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        if self.inverse:\n",
    "            seq_y = np.concatenate([self.data_x[r_begin:r_begin+self.label_len], self.data_y[r_begin+self.label_len:r_end]], 0)\n",
    "        else:\n",
    "            seq_y = self.data_x[r_begin:r_end]\n",
    "\n",
    "        # Getting sequences for time encoding\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        # Debugging print to check for NaN values in the sequences\n",
    "        if np.isnan(seq_x).any() or np.isnan(seq_y).any():\n",
    "            print(f\"NaN values detected at index {index}\")\n",
    "        \n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in the dataset, replacing with forward fill (ffill)\n",
      "dates                      date  month  day  weekday  hour  minute\n",
      "0     2023-02-09 14:30:00      2    9        3    14       2\n",
      "1     2023-02-09 14:35:00      2    9        3    14       2\n",
      "2     2023-02-09 14:40:00      2    9        3    14       2\n",
      "3     2023-02-09 14:45:00      2    9        3    14       3\n",
      "4     2023-02-09 14:50:00      2    9        3    14       3\n",
      "...                   ...    ...  ...      ...   ...     ...\n",
      "22703 2024-04-04 15:20:00      4    4        3    15       1\n",
      "22704 2024-04-04 15:25:00      4    4        3    15       1\n",
      "22705 2024-04-04 15:30:00      4    4        3    15       2\n",
      "22706 2024-04-04 15:35:00      4    4        3    15       2\n",
      "22707 2024-04-04 15:40:00      4    4        3    15       2\n",
      "\n",
      "[22708 rows x 6 columns]\n",
      "[[ 2  9  3 14]\n",
      " [ 2  9  3 14]\n",
      " [ 2  9  3 14]\n",
      " ...\n",
      " [ 4  4  3 15]\n",
      " [ 4  4  3 15]\n",
      " [ 4  4  3 15]]\n",
      "NaN values found in the dataset, replacing with forward fill (ffill)\n",
      "dates                      date  month  day  weekday  hour  minute\n",
      "22612 2024-04-03 14:20:00      4    3        2    14       1\n",
      "22613 2024-04-03 14:25:00      4    3        2    14       1\n",
      "22614 2024-04-03 14:30:00      4    3        2    14       2\n",
      "22615 2024-04-03 14:35:00      4    3        2    14       2\n",
      "22616 2024-04-03 14:40:00      4    3        2    14       2\n",
      "...                   ...    ...  ...      ...   ...     ...\n",
      "25948 2024-06-03 15:50:00      6    3        0    15       3\n",
      "25949 2024-06-03 15:55:00      6    3        0    15       3\n",
      "25950 2024-06-03 16:00:00      6    3        0    16       0\n",
      "25951 2024-06-03 16:05:00      6    3        0    16       0\n",
      "25952 2024-06-03 16:10:00      6    3        0    16       0\n",
      "\n",
      "[3341 rows x 6 columns]\n",
      "[[ 4  3  2 14]\n",
      " [ 4  3  2 14]\n",
      " [ 4  3  2 14]\n",
      " ...\n",
      " [ 6  3  0 16]\n",
      " [ 6  3  0 16]\n",
      " [ 6  3  0 16]]\n",
      "NaN values found in the dataset, replacing with forward fill (ffill)\n",
      "dates                      date  month  day  weekday  hour  minute\n",
      "25857 2024-05-31 14:50:00      5   31        4    14       3\n",
      "25858 2024-05-31 14:55:00      5   31        4    14       3\n",
      "25859 2024-05-31 15:00:00      5   31        4    15       0\n",
      "25860 2024-05-31 15:05:00      5   31        4    15       0\n",
      "25861 2024-05-31 15:10:00      5   31        4    15       0\n",
      "...                   ...    ...  ...      ...   ...     ...\n",
      "32436 2024-09-30 19:40:00      9   30        0    19       2\n",
      "32437 2024-09-30 19:45:00      9   30        0    19       3\n",
      "32438 2024-09-30 19:50:00      9   30        0    19       3\n",
      "32439 2024-09-30 19:55:00      9   30        0    19       3\n",
      "32440 2024-09-30 20:00:00      9   30        0    20       0\n",
      "\n",
      "[6584 rows x 6 columns]\n",
      "[[ 5 31  4 14]\n",
      " [ 5 31  4 14]\n",
      " [ 5 31  4 15]\n",
      " ...\n",
      " [ 9 30  0 19]\n",
      " [ 9 30  0 19]\n",
      " [ 9 30  0 20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1740751821.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1740751821.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1740751821.py:50: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_raw.fillna(method='ffill', inplace=True)  # Replace NaNs with forward fill\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:203: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:204: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:205: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:206: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
      "C:\\Users\\UseR\\AppData\\Local\\Temp\\ipykernel_528\\1097067871.py:207: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    Trading_dataset(flag='train', root_path=r\"C:\\Users\\UseR\\Desktop\\development\\trading_tasks\\stock-prediction-micn\\data\\ETT\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True)\n",
    "\n",
    "vali_loader = DataLoader(\n",
    "    Trading_dataset(flag='val', root_path=\"data/ETT/\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Trading_dataset(flag='test', root_path=\"data/ETT/\", data_path=\"5m_intraday_data.csv\", freq=FREQ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4; hour_size = 24\n",
    "        weekday_size = 7; day_size = 32; month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
    "        if freq=='t':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        \n",
    "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:,:,3])\n",
    "        weekday_x = self.weekday_embed(x[:,:,2])\n",
    "        day_x = self.day_embed(x[:,:,1])\n",
    "        month_x = self.month_embed(x[:,:,0])\n",
    "        \n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,channels\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class series_decomp_multi(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp_multi, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.moving_avg = [moving_avg(kernel, stride=1) for kernel in kernel_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = []\n",
    "        res = []\n",
    "        for func in self.moving_avg:\n",
    "            moving_avg = func(x)\n",
    "            moving_mean.append(moving_avg)\n",
    "            sea = x - moving_avg\n",
    "            res.append(sea)\n",
    "\n",
    "        sea = sum(res) / len(res)\n",
    "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
    "        return sea, moving_mean\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        self.initialize_weight(self.layer1)\n",
    "        self.initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_weight(self, x):\n",
    "        nn.init.xavier_uniform_(x.weight)\n",
    "        if x.bias is not None:\n",
    "            nn.init.constant_(x.bias, 0)\n",
    "\n",
    "\n",
    "class MIC(nn.Module):\n",
    "    \"\"\"\n",
    "    MIC layer to extract local and global features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24], isometric_kernel=[18, 6], device='cuda'):\n",
    "        super(MIC, self).__init__()\n",
    "        self.src_mask = None\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.isometric_kernel = isometric_kernel\n",
    "        self.device = device\n",
    "        \n",
    "        # isometric convolution\n",
    "        self.isometric_conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                   kernel_size=i,padding=0,stride=1)\n",
    "                                        for i in isometric_kernel])\n",
    "\n",
    "        # downsampling convolution: padding=i//2, stride=i\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                             kernel_size=i,padding=i//2,stride=i)\n",
    "                                  for i in conv_kernel])\n",
    "\n",
    "        # upsampling convolution\n",
    "        self.conv_trans = nn.ModuleList([nn.ConvTranspose1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                            kernel_size=i,padding=0,stride=i)\n",
    "                                        for i in conv_kernel])\n",
    "\n",
    "        self.decomp = nn.ModuleList([series_decomp(k) for k in decomp_kernel])\n",
    "        self.merge = torch.nn.Conv2d(in_channels=feature_size, out_channels=feature_size, kernel_size=(len(self.conv_kernel), 1))\n",
    "\n",
    "        self.fnn = FeedForwardNetwork(feature_size, feature_size*4, dropout)\n",
    "        self.fnn_norm = torch.nn.LayerNorm(feature_size)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(feature_size)\n",
    "        self.act = torch.nn.Tanh()\n",
    "        self.drop = torch.nn.Dropout(0.05)\n",
    "\n",
    "    def conv_trans_conv(self, input, conv1d, conv1d_trans, isometric):\n",
    "        batch, seq_len, channel = input.shape\n",
    "        x = input.permute(0, 2, 1)\n",
    "\n",
    "        # downsampling convolution\n",
    "        x1 = self.drop(self.act(conv1d(x)))\n",
    "        x = x1\n",
    "\n",
    "        # isometric convolution \n",
    "        zeros = torch.zeros((x.shape[0], x.shape[1], x.shape[2]-1), device=self.device)\n",
    "        x = torch.cat((zeros, x), dim=-1)\n",
    "        x = self.drop(self.act(isometric(x)))\n",
    "        x = self.norm((x+x1).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        # upsampling convolution\n",
    "        x = self.drop(self.act(conv1d_trans(x)))\n",
    "        x = x[:, :, :seq_len]   # truncate\n",
    "\n",
    "        x = self.norm(x.permute(0, 2, 1) + input)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        # multi-scale\n",
    "        multi = []  \n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            src_out, trend1 = self.decomp[i](src)\n",
    "            src_out = self.conv_trans_conv(src_out, self.conv[i], self.conv_trans[i], self.isometric_conv[i])\n",
    "            multi.append(src_out)  \n",
    "\n",
    "        # merge\n",
    "        mg = torch.tensor([], device = self.device)\n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            mg = torch.cat((mg, multi[i].unsqueeze(1)), dim=1)\n",
    "        mg = self.merge(mg.permute(0,3,1,2)).squeeze(-2).permute(0,2,1)\n",
    "        \n",
    "        return self.fnn_norm(mg + self.fnn(mg))\n",
    "\n",
    "\n",
    "class Seasonal_Prediction(nn.Module):\n",
    "    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1,\n",
    "                conv_kernel=[2, 4], isometric_kernel=[18, 6], device='cuda'):\n",
    "        super(Seasonal_Prediction, self).__init__()\n",
    "\n",
    "        self.mic = nn.ModuleList([MIC(feature_size=embedding_size, n_heads=n_heads,\n",
    "                                                   decomp_kernel=decomp_kernel,conv_kernel=conv_kernel, isometric_kernel=isometric_kernel, device=device)\n",
    "                                      for i in range(d_layers)])\n",
    "\n",
    "        self.projection = nn.Linear(embedding_size, c_out)\n",
    "\n",
    "    def forward(self, dec):\n",
    "        for mic_layer in self.mic:\n",
    "            dec = mic_layer(dec)\n",
    "        return self.projection(dec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICN(nn.Module):\n",
    "    def __init__(self, dec_in, c_out, seq_len, label_len, out_len,\n",
    "                 d_model=512, n_heads=8,d_layers=2,\n",
    "                 dropout=0.0,embed='fixed', freq='h',\n",
    "                 device=torch.device('cuda:0'), mode='regre',\n",
    "                 decomp_kernel=[33], conv_kernel=[12, 24], isometric_kernel=[18, 6],):\n",
    "        super(MICN, self).__init__()\n",
    "\n",
    "        self.pred_len = out_len\n",
    "        self.seq_len = seq_len\n",
    "        self.c_out = c_out\n",
    "        self.decomp_kernel = decomp_kernel\n",
    "        self.mode = mode\n",
    "\n",
    "        self.decomp_multi = series_decomp_multi(decomp_kernel)\n",
    "\n",
    "        # embedding\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
    "\n",
    "        self.conv_trans = Seasonal_Prediction(embedding_size=d_model, n_heads=n_heads, dropout=dropout,\n",
    "                                     d_layers=d_layers, decomp_kernel=decomp_kernel, c_out=c_out, conv_kernel=conv_kernel,\n",
    "                                     isometric_kernel=isometric_kernel, device=device)\n",
    "\n",
    "        self.regression = nn.Linear(seq_len, out_len)\n",
    "        self.regression.weight = nn.Parameter((1/out_len) * torch.ones([out_len, seq_len]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        # trend-cyclical prediction block: regre or mean\n",
    "        if self.mode == 'regre':\n",
    "            seasonal_init_enc, trend = self.decomp_multi(x_enc)\n",
    "            trend = self.regression(trend.permute(0,2,1)).permute(0, 2, 1)\n",
    "        elif self.mode == 'mean':\n",
    "            mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "            seasonal_init_enc, trend = self.decomp_multi(x_enc)\n",
    "            trend = torch.cat([trend[:, -self.seq_len:, :], mean], dim=1)\n",
    "\n",
    "        # embedding\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
    "        seasonal_init_dec = torch.cat([seasonal_init_enc[:, -self.seq_len:, :], zeros], dim=1)\n",
    "        dec_out = self.dec_embedding(seasonal_init_dec, x_mark_dec)\n",
    "\n",
    "        dec_out = self.conv_trans(dec_out)\n",
    "        dec_out = dec_out[:, -self.pred_len:, :] + trend[:, -self.pred_len:, :]\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Started Model training =====\n",
      "batch_x shape: torch.Size([32, 96, 1])\n",
      "batch_y shape: torch.Size([32, 192, 1])\n",
      "batch_x_mark shape: torch.Size([32, 96, 4])\n",
      "batch_y_mark shape: torch.Size([32, 192, 4])\n",
      "dec_inp shape: torch.Size([32, 192, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (13) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvali_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust epochs as needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[417], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, vali_loader, epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y, batch_x_mark, batch_y_mark) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     58\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m     pred, true \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, true)\n\u001b[0;32m     61\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[417], line 102\u001b[0m, in \u001b[0;36mprocess_one_batch\u001b[1;34m(model, batch_x, batch_y, batch_x_mark, batch_y_mark)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdec_inp shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dec_inp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Get model outputs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, batch_y[:, \u001b[38;5;241m-\u001b[39mPRED_LEN:, :]\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[416], line 44\u001b[0m, in \u001b[0;36mMICN.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[0;32m     41\u001b[0m seasonal_init_dec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([seasonal_init_enc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len:, :], zeros], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(seasonal_init_dec, x_mark_dec)\n\u001b[1;32m---> 44\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_trans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :] \u001b[38;5;241m+\u001b[39m trend[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[415], line 175\u001b[0m, in \u001b[0;36mSeasonal_Prediction.forward\u001b[1;34m(self, dec)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dec):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mic_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmic:\n\u001b[1;32m--> 175\u001b[0m         dec \u001b[38;5;241m=\u001b[39m \u001b[43mmic_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(dec)\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UseR\\.conda\\envs\\micn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[415], line 150\u001b[0m, in \u001b[0;36mMIC.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_kernel)):\n\u001b[0;32m    149\u001b[0m     src_out, trend1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomp[i](src)\n\u001b[1;32m--> 150\u001b[0m     src_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_trans_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_trans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misometric_conv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     multi\u001b[38;5;241m.\u001b[39mappend(src_out)  \n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# merge\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[415], line 135\u001b[0m, in \u001b[0;36mMIC.conv_trans_conv\u001b[1;34m(self, input, conv1d, conv1d_trans, isometric)\u001b[0m\n\u001b[0;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((zeros, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(isometric(x)))\n\u001b[1;32m--> 135\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm((\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mx1\u001b[49m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# upsampling convolution\u001b[39;00m\n\u001b[0;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(conv1d_trans(x)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (13) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "    model_dict = {\n",
    "        'micn': MICN,\n",
    "    }\n",
    "    model = model_dict[MODEL](\n",
    "        DEC_IN,\n",
    "        C_OUT, \n",
    "        SEQ_LEN, \n",
    "        LABEL_LEN,\n",
    "        PRED_LEN,\n",
    "        D_MODEL, \n",
    "        N_HEADS,\n",
    "        D_LAYERS,\n",
    "        DROPOUT,\n",
    "        EMBED,\n",
    "        FREQ,\n",
    "        DEVICE,\n",
    "        MODE,\n",
    "        DECOMP_KERNEL,\n",
    "        CONV_KERNEL,\n",
    "        ISOMETRIC_KERNEL\n",
    "    ).float()\n",
    "    return model\n",
    "\n",
    "# Optimizer and loss function\n",
    "def select_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "def select_criterion():\n",
    "    return nn.MSELoss()\n",
    "\n",
    "# Validation function\n",
    "def validate(model, vali_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:\n",
    "            pred, true = process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "            loss = criterion(pred, true)\n",
    "            total_loss.append(loss.item())\n",
    "    return np.average(total_loss)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, vali_loader, epochs=100):\n",
    "    optimizer = select_optimizer(model)\n",
    "    criterion = select_criterion()\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)  \n",
    "\n",
    "    print('===== Started Model training =====')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        iter_count = 0\n",
    "        time_now = time.time()\n",
    "        \n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred, true = process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "            loss = criterion(pred, true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            iter_count += 1  \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"\\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f}\")\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((epochs - epoch - 1) * len(train_loader) - i)\n",
    "                print(f'\\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')\n",
    "                iter_count = 0  \n",
    "                time_now = time.time()  \n",
    "\n",
    "        avg_train_loss = np.average(train_loss)\n",
    "        vali_loss = validate(model, vali_loader, criterion)\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {vali_loss:.4f}')\n",
    "\n",
    "        early_stopping(vali_loss, model, \"checkpoints\")\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "def process_one_batch(model, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    batch_x = batch_x.float()\n",
    "    batch_y = batch_y.float() \n",
    "    batch_x_mark = batch_x_mark.float() \n",
    "    batch_y_mark = batch_y_mark.float() \n",
    "\n",
    "    # Prepare decoder input\n",
    "    # Ensure dec_inp has the correct size for the model\n",
    "    dec_inp = torch.zeros([batch_y.shape[0], PRED_LEN, batch_y.shape[-1]]).float()  # Adjust padding if needed\n",
    "    dec_inp = torch.cat([batch_y[:, :LABEL_LEN, :], dec_inp], dim=1)\n",
    "\n",
    "    # Check shapes\n",
    "    print(\"batch_x shape:\", batch_x.shape)\n",
    "    print(\"batch_y shape:\", batch_y.shape)\n",
    "    print(\"batch_x_mark shape:\", batch_x_mark.shape)\n",
    "    print(\"batch_y_mark shape:\", batch_y_mark.shape)\n",
    "    print(\"dec_inp shape:\", dec_inp.shape)\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    return outputs, batch_y[:, -PRED_LEN:, :]  # Returnin\n",
    "\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_model()\n",
    "\n",
    "# Train model\n",
    "train(model, train_loader, vali_loader, epochs=10)  # Adjust epochs as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
