{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Metrics?\n",
    "\n",
    "A metric is a numerical value that quantifies the quality of a model's predictions. For example, `accuracy` is a metric used to assess the performance of a classifier model. \n",
    "\n",
    "## How can we log metrics with MLflow? \n",
    "\n",
    "In this case we can use `mlflow.log_metric` or `mlflow.log_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_for_ml_dev.utils.utils import get_root_project\n",
    "import mlflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and set experiment \n",
    "experiment_name = \"metrics\"\n",
    "experiment = mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 22339bc513c24caea89a89b315a6391b\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"logging metrics\") as run:\n",
    "    print(f\"Run: {run.info.run_id}\")\n",
    "\n",
    "    # log metrics\n",
    "    mlflow.log_metric(\"accuracy\", 0.9)\n",
    "\n",
    "    # log metrics \n",
    "    mlflow.log_metrics({\"precision\": 0.8, \"recall\": 0.7, \"f1\": 0.75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import make_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* eval_fn –\n",
    "\n",
    "A function that computes the metric with the following signature:\n",
    "```python\n",
    "def eval_fn(\n",
    "    predictions: pandas.Series,\n",
    "    targets: pandas.Series,\n",
    "    metrics: Dict[str, MetricValue],\n",
    "    **kwargs,\n",
    ") -> Union[float, MetricValue]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predictions: A pandas Series containing the predictions made by the model.\n",
    "        targets: (Optional) A pandas Series containing the corresponding labels\n",
    "            for the predictions made on that input.\n",
    "        metrics: (Optional) A dictionary containing the metrics calculated by the\n",
    "            default evaluator.  The keys are the names of the metrics and the values\n",
    "            are the metric values.  To access the MetricValue for the metrics\n",
    "            calculated by the system, make sure to specify the type hint for this\n",
    "            parameter as Dict[str, MetricValue].  Refer to the DefaultEvaluator\n",
    "            behavior section for what metrics will be returned based on the type of\n",
    "            model (i.e. classifier or regressor).  kwargs: Includes a list of args\n",
    "            that are used to compute the metric. These args could information coming\n",
    "            from input data, model outputs or parameters specified in the\n",
    "            `evaluator_config` argument of the `mlflow.evaluate` API.\n",
    "        kwargs: Includes a list of args that are used to compute the metric. These\n",
    "            args could be information coming from input data, model outputs,\n",
    "            other metrics, or parameters specified in the `evaluator_config`\n",
    "            argument of the `mlflow.evaluate` API.\n",
    "\n",
    "    Returns: MetricValue with per-row scores, per-row justifications, and aggregate\n",
    "        results.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "* greater_is_better – Whether a higher value of the metric is better.\n",
    "\n",
    "* name – The name of the metric. This argument must be specified if eval_fn is a lambda function or the eval_fn.__name__ attribute is not available.\n",
    "\n",
    "* long_name – (Optional) The long name of the metric. For example, \"mean_squared_error\" for \"mse\".\n",
    "\n",
    "* version – (Optional) The metric version. For example v1.\n",
    "\n",
    "* metric_details – (Optional) A description of the metric and how it is calculated.\n",
    "\n",
    "* metric_metadata – (Optional) A dictionary containing metadata for the metric.\n",
    "\n",
    "* genai_metric_args – (Optional) A dictionary containing arguments specified by users when calling make_genai_metric or make_genai_metric_from_prompt. Those args are persisted so that we can deserialize the same metric object later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_diff_plus_one(eval_df, _builtin_metrics):\n",
    "    \"\"\"\n",
    "    This example custom metric function creates a metric based on the ``prediction`` and\n",
    "    ``target`` columns in ``eval_df`.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(eval_df[\"prediction\"] - eval_df[\"target\"] + 1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric = make_metric(\n",
    "        eval_fn=squared_diff_plus_one,\n",
    "        greater_is_better=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_regression(n_samples=100, n_features=1, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(x, y)\n",
    "\n",
    "y_pred = regressor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-22.116520</td>\n",
       "      <td>-22.093833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.948722</td>\n",
       "      <td>92.964142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-49.510976</td>\n",
       "      <td>-50.315632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.065630</td>\n",
       "      <td>61.358347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.184361</td>\n",
       "      <td>72.554530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction     target\n",
       "0  -22.116520 -22.093833\n",
       "1   92.948722  92.964142\n",
       "2  -49.510976 -50.315632\n",
       "3   61.065630  61.358347\n",
       "4   73.184361  72.554530"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame({\"prediction\": y_pred, \"target\": y})\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358.3227485317362"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_diff_plus_one(eval_df, _builtin_metrics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_metric.greater_is_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\projects\\mlflow_for_ml_dev\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "2024/08/15 12:32:37 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/08/15 12:32:37 INFO mlflow.models.evaluation.default_evaluator: Shap explainer ExactExplainer is used.\n",
      "2024/08/15 12:32:37 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: IndexError('list index out of range'). Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "results = mlflow.evaluate(data = eval_df, extra_metrics=[custom_metric], model_type = \"regressor\", predictions=\"target\", targets=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_count': 100,\n",
       " 'mean_absolute_error': 0.73900032740808,\n",
       " 'mean_squared_error': 2.5245445692752297,\n",
       " 'root_mean_squared_error': 1.5888815466469581,\n",
       " 'sum_on_target': 2163.4540178832294,\n",
       " 'mean_on_target': 21.634540178832296,\n",
       " 'r2_score': 0.9996189126086862,\n",
       " 'max_error': 10.206270195868342,\n",
       " 'mean_absolute_percentage_error': 0.018866046976628085,\n",
       " 'squared_diff_plus_one': 346.5861653233097}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
